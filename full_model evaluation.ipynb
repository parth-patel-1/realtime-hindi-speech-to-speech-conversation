{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6320342c-fc30-460c-9f5d-bef73c07d1fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flash attention 2 is not installed\n",
      "ParlerTTSForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "ParlerTTSForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n",
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import queue\n",
    "import time\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "from openai import OpenAI\n",
    "\n",
    "# === YOUR API CLIENT ===\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.sambanova.ai/v1\",\n",
    "    api_key=\"4a81daa9-5f3d-409b-9f30-ebedb379219a\"\n",
    ")\n",
    "\n",
    "# === Device & Models ===\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TTS setup\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"ai4bharat/indic-parler-tts\")\n",
    "MODEL_1   = ParlerTTSForConditionalGeneration.from_pretrained(\"ai4bharat/indic-parler-tts\").to(DEVICE)\n",
    "MODEL_2   = ParlerTTSForConditionalGeneration.from_pretrained(\"ai4bharat/indic-parler-tts\").to(DEVICE)\n",
    "\n",
    "# Voice conditioning\n",
    "DESC_TOKENIZER = AutoTokenizer.from_pretrained(MODEL_1.config.text_encoder._name_or_path)\n",
    "DESCRIPTION    = \"Rohit's voice is monotone yet slightly fast in delivery, with minimal background noise.\"\n",
    "DESC_INPUTS    = DESC_TOKENIZER(DESCRIPTION, return_tensors=\"pt\").to(DEVICE)\n",
    "DESC_INPUT_IDS = DESC_INPUTS.input_ids\n",
    "DESC_ATTN_MASK = DESC_INPUTS.attention_mask\n",
    "\n",
    "SAMPLING_RATE = MODEL_1.config.sampling_rate\n",
    "sd.default.latency = 'low'\n",
    "\n",
    "# === TTS Worker ===\n",
    "def tts_worker(text_queue, audio_dict, model, audio_events):\n",
    "    while True:\n",
    "        item = text_queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "        seq_id, chunk, start_time = item\n",
    "\n",
    "        inp = TOKENIZER(chunk, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            aud = model.generate(\n",
    "                input_ids=DESC_INPUT_IDS,\n",
    "                attention_mask=DESC_ATTN_MASK,\n",
    "                prompt_input_ids=inp.input_ids,\n",
    "                prompt_attention_mask=inp.attention_mask\n",
    "            )\n",
    "        audio = aud.cpu().numpy().squeeze()\n",
    "        if audio.ndim>1: audio = audio.flatten()\n",
    "        audio = audio.astype(np.float32)\n",
    "        audio /= np.max(np.abs(audio)) or 1.0\n",
    "\n",
    "        audio_dict[seq_id] = (audio, start_time)\n",
    "        audio_events[seq_id].set()\n",
    "\n",
    "# === Playback Thread ===\n",
    "def playback_thread_fn(audio_dict, audio_events, chunk_ids, done_stream):\n",
    "    next_seq = 0\n",
    "    while True:\n",
    "        while next_seq >= len(chunk_ids):\n",
    "            if done_stream.is_set(): break\n",
    "            time.sleep(0.01)\n",
    "        if next_seq >= len(chunk_ids) and done_stream.is_set():\n",
    "            break\n",
    "\n",
    "        audio_events[next_seq].wait()\n",
    "        audio, st = audio_dict[next_seq]\n",
    "        if next_seq==0:\n",
    "            latency = time.time() - st\n",
    "            print(f\"\\n Latency: {latency:.2f}s\")\n",
    "        sd.play(audio, samplerate=SAMPLING_RATE)\n",
    "        sd.wait()\n",
    "        next_seq += 1\n",
    "\n",
    "    print(\"All done.\")\n",
    "\n",
    "# === Streaming + Dynamic Chunking + TTS ===\n",
    "def stream_llm_tts(prompt, initial_size=10, max_size=1000):\n",
    "    text_q1, text_q2 = queue.Queue(), queue.Queue()\n",
    "    audio_dict, audio_events = {}, {}\n",
    "    chunk_ids = []\n",
    "    lock = threading.Lock()\n",
    "    done_stream = threading.Event()\n",
    "\n",
    "    # start TTS threads\n",
    "    t1 = threading.Thread(target=tts_worker, args=(text_q1, audio_dict, MODEL_1, audio_events), daemon=True)\n",
    "    t2 = threading.Thread(target=tts_worker, args=(text_q2, audio_dict, MODEL_2, audio_events), daemon=True)\n",
    "    t1.start(); t2.start()\n",
    "    # start playback\n",
    "    pb = threading.Thread(target=playback_thread_fn, args=(audio_dict, audio_events, chunk_ids, done_stream), daemon=True)\n",
    "    pb.start()\n",
    "\n",
    "    buffer = \"\"\n",
    "    toggle = 0\n",
    "    sizes  = [initial_size, initial_size]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Meta-Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\":\"system\", \"content\":\"You are a Hindi-only assistant. Keep responses short.\"},\n",
    "            {\"role\":\"user\",   \"content\":prompt}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    # stream tokens\n",
    "    for chunk in response:\n",
    "        delta = chunk.choices[0].delta\n",
    "        if hasattr(delta, \"content\") and delta.content:\n",
    "            # print text live\n",
    "            print(delta.content, end=\"\", flush=True)\n",
    "\n",
    "            buffer += delta.content\n",
    "            # emit full chunks\n",
    "            while len(buffer) >= sizes[toggle % 2]:\n",
    "                txt = buffer[:sizes[toggle % 2]].strip()\n",
    "                buffer = buffer[sizes[toggle % 2]:]\n",
    "\n",
    "                with lock:\n",
    "                    seq = len(chunk_ids)\n",
    "                    chunk_ids.append(seq)\n",
    "                    audio_events[seq] = threading.Event()\n",
    "\n",
    "                # dispatch chunk\n",
    "                (text_q1 if seq % 2 == 0 else text_q2).put((seq, txt, time.time()))\n",
    "\n",
    "                # update sizes\n",
    "                sizes[toggle % 2] = min(sizes[toggle % 2] * 2, max_size)\n",
    "                toggle += 1\n",
    "\n",
    "    # leftover\n",
    "    if buffer.strip():\n",
    "        print(buffer, end=\"\", flush=True)\n",
    "        with lock:\n",
    "            seq = len(chunk_ids)\n",
    "            chunk_ids.append(seq)\n",
    "            audio_events[seq] = threading.Event()\n",
    "        (text_q1 if seq % 2 == 0 else text_q2).put((seq, buffer.strip(), time.time()))\n",
    "\n",
    "    # signal end\n",
    "    text_q1.put(None); text_q2.put(None)\n",
    "    done_stream.set()\n",
    "\n",
    "    t1.join(); t2.join(); pb.join()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "683f7c37-f6ac-4453-9e3f-2db85798fd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "एक छोटे से गाँव में एक लड़का रहता था। वह एक दिन एक पुरानी किताब में एक रहस्यमय पत्र पढ़ा। वह पत्र उसे एक पुराने मंदिर में ले जाता था। वहां, वह एक सुंदर स्त्री को देखता है जो उसके भविष्य को बताती है।ं ले जाता था। वहां, वह एक सुंदर स्त्री को देखता है जो उसके भविष्य को बताती है।"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`prompt_attention_mask` is specified but `attention_mask` is not. A full `attention_mask` will be created. Make sure this is the intended behaviour.\n",
      "`prompt_attention_mask` is specified but `attention_mask` is not. A full `attention_mask` will be created. Make sure this is the intended behaviour.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[play #0] Latency: 1.96s\n",
      "All done.\n",
      "\n",
      "Finished streaming & playback.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me a short story\" \n",
    "stream_llm_tts(prompt, initial_size=20, max_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8913e457-340c-4377-bfaf-20a9631b5ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RealtimeSTT import AudioToTextRecorder\n",
    "print(\"Wait until it says 'speak now'\")\n",
    "import time\n",
    "\n",
    "recorder = AudioToTextRecorder(model=\"medium\",print_transcription_time=True,language=\"hi\",spinner=False)\n",
    "def pr(text):\n",
    "    print(text)\n",
    "    \n",
    "while True:\n",
    "    # recorder.text(speak_from_prompt)\n",
    "    recorder.text(stream_llm_tts)\n",
    "    #small, large\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a9f7d7-6b02-4071-98ec-3fa963ea2c86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499ca7e8-989e-45e6-829e-59d76ab34b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash attention 2 is not installed\n",
      "ParlerTTSForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "ParlerTTSForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n",
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n",
      "Wait until it says 'speak now'\u001b[0m\n",
      "RealTimeSTT: realtimestt - WARNING - Audio queue size exceeds latency limit. Current size: 377. Discarding old audio chunks.\n",
      "\u001b[0m\u001b[0mModel medium completed transcription in 0.04 seconds\u001b[0m\n",
      "\u001b[0mक\u001b[0m\u001b[0m्या मैं आपकी सहायता कर सकता हूँ?\u001b[0m\u001b[0mूँ?\u001b[0m\u001b[0mWARNING:parler_tts.modeling_parler_tts:`prompt_attention_mask` is specified but `attention_mask` is not. A full `attention_mask` will be created. Make sure this is the intended behaviour.\n",
      "\u001b[0m\n",
      " Latency: 2.34s\u001b[0m\n",
      "\u001b[0mModel medium completed transcription in 0.26 seconds\u001b[0m\n",
      "\u001b[0mग\u001b[0m\u001b[0mुगल एक कंप्यूटर प्रोग्राम है जो इंटरन\u001b[0m\u001b[0mेट पर जानकारी को संग्रहीत और प्रदान करता है।\u001b[0m\u001b[0m यह काम करता है:\n",
      "\n",
      "1. उपयोगकर्ता के प्रश्न\u001b[0m\u001b[0mों को पढ़ता है।\n",
      "2. जानकारी को खोजता\u001b[0m\u001b[0m है और उपयोगकर्ता के प्रश्नों के अनुसार स\u001b[0m\u001b[0mंग्रहीत होती है।\n",
      "3. उपयोगकर्ता को जानकार\u001b[0m\u001b[0mी के साथ संबंधित लिंक और वेबसाइट प्रदान करत\u001b[0m\u001b[0mा है।\n",
      "4. उपयोगकर्ता के प्रश्नों को संब\u001b[0m\u001b[0mंधित वेबसाइटों पर भेजता है।\u001b[0m\u001b[0mा के प्रश्नों को संबंधित वेबसाइटों पर भेजता है।\u001b[0m\u001b[0m\n",
      " Latency: 2.49s\u001b[0m\n",
      "Exception in thread \u001b[0mThread-24 (tts_worker)\u001b[0m:\n",
      "\u001b[0mTraceback (most recent call last):\n",
      "\u001b[0m  File \"/home/irlab/.pyenv/versions/3.10.13/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "\u001b[0m    \u001b[0mself.run()\u001b[0m\n",
      "\u001b[0m  File \"/home/irlab/.pyenv/versions/3.10.13/lib/python3.10/threading.py\", line 953, in run\n",
      "\u001b[0m    \u001b[0mself._target(*self._args, **self._kwargs)\u001b[0m\n",
      "\u001b[0m  File \"/media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254e15/Parth/Voice/model.py\", line 48, in tts_worker\n",
      "\u001b[0m    \u001b[0maud = model.generate(\u001b[0m\n",
      "\u001b[0m  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[0m    \u001b[0mreturn func(*args, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/parler_tts/modeling_parler_tts.py\", line 3637, in generate\n",
      "\u001b[0m    \u001b[0msample = self.audio_encoder.decode(audio_codes=sample[None, ...], **single_audio_decode_kwargs).audio_values\u001b[0m\n",
      "\u001b[0m  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/transformers/models/dac/modeling_dac.py\", line 672, in decode\n",
      "\u001b[0m    \u001b[0maudio_values = self.decoder(quantized_representation).squeeze(1)\u001b[0m\n",
      "\u001b[0m  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "\u001b[0m    \u001b[0mreturn self._call_impl(*args, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "\u001b[0m    \u001b[0mreturn forward_call(*args, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/transformers/models/dac/modeling_dac.py\", line 440, in forward\n",
      "\u001b[0m    \u001b[0mhidden_state = layer(hidden_state)\u001b[0m\n",
      "\u001b[0m  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "\u001b[0m    \u001b[0mreturn self._call_impl(*args, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "\u001b[0m    \u001b[0mreturn forward_call(*args, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/transformers/models/dac/modeling_dac.py\", line 267, in forward\n",
      "\u001b[0m    \u001b[0mhidden_state = self.res_unit1(hidden_state)\u001b[0m\n",
      "\u001b[0m  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "\u001b[0m    \u001b[0mreturn self._call_impl(*args, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "\u001b[0m    \u001b[0mreturn forward_call(*args, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/transformers/models/dac/modeling_dac.py\", line 210, in forward\n",
      "\u001b[0m    \u001b[0moutput_tensor = self.conv2(self.snake2(output_tensor))\u001b[0m\n",
      "\u001b[0m  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "\u001b[0m    \u001b[0mreturn self._call_impl(*args, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "\u001b[0m    \u001b[0mreturn forward_call(*args, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/transformers/models/dac/modeling_dac.py\", line 107, in forward\n",
      "\u001b[0m    \u001b[0mhidden_states = hidden_states + (self.alpha + 1e-9).reciprocal() * torch.sin(self.alpha * hidden_states).pow(2)\u001b[0m\n",
      "\u001b[0mtorch\u001b[0m.\u001b[0mOutOfMemoryError\u001b[0m: \u001b[0mCUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 10.56 GiB of which 108.38 MiB is free. Including non-PyTorch memory, this process has 9.60 GiB memory in use. Of the allocated memory 7.63 GiB is allocated by PyTorch, and 179.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\u001b[0m\n",
      "\u001b[0mERROR:root:General error in transcription: CUDA failed with error out of memory\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/RealtimeSTT/audio_recorder.py\", line 218, in run\n",
      "    transcription = \" \".join(seg.text for seg in segments).strip()\n",
      "  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/RealtimeSTT/audio_recorder.py\", line 218, in <genexpr>\n",
      "    transcription = \" \".join(seg.text for seg in segments).strip()\n",
      "  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/faster_whisper/transcribe.py\", line 553, in _batched_segments_generator\n",
      "    results = self.forward(\n",
      "  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/faster_whisper/transcribe.py\", line 121, in forward\n",
      "    encoder_output, outputs = self.generate_segment_batched(\n",
      "  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/faster_whisper/transcribe.py\", line 210, in generate_segment_batched\n",
      "    encoder_output = self.model.encode(features)\n",
      "  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/faster_whisper/transcribe.py\", line 1358, in encode\n",
      "    return self.model.encode(features, to_cpu=to_cpu)\n",
      "RuntimeError: CUDA failed with error out of memory\n",
      "\u001b[0mRealTimeSTT: realtimestt - ERROR - Transcription error: CUDA failed with error out of memory\n",
      "\u001b[0mRealTimeSTT: realtimestt - ERROR - Error during transcription: CUDA failed with error out of memory\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/RealtimeSTT/audio_recorder.py\", line 1549, in perform_final_transcription\n",
      "    raise Exception(result)\n",
      "Exception: CUDA failed with error out of memory\n",
      "\u001b[0m\u001b[0mTraceback (most recent call last):\n",
      "  File \"/media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254e15/Parth/Voice/model.py\", line 162, in <module>\n",
      "    recorder.text(stream_llm_tts)\n",
      "  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/RealtimeSTT/audio_recorder.py\", line 1671, in text\n",
      "    args=(self.transcribe(),)).start()\n",
      "  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/RealtimeSTT/audio_recorder.py\", line 1588, in transcribe\n",
      "    return self.perform_final_transcription(audio_copy)\n",
      "  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/RealtimeSTT/audio_recorder.py\", line 1552, in perform_final_transcription\n",
      "    raise e\n",
      "  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/RealtimeSTT/audio_recorder.py\", line 1549, in perform_final_transcription\n",
      "    raise Exception(result)\n",
      "Exception: CUDA failed with error out of memory\n",
      "^C\n",
      "\u001b[0mException ignored in: \u001b[0m<module 'threading' from '/home/irlab/.pyenv/versions/3.10.13/lib/python3.10/threading.py'>\u001b[0m\n",
      "\u001b[0mTraceback (most recent call last):\n",
      "\u001b[0m  File \"/home/irlab/.pyenv/versions/3.10.13/lib/python3.10/threading.py\", line 1567, in _shutdown\n",
      "\u001b[0m    \u001b[0mlock.acquire()\u001b[0m\n",
      "\u001b[0mKeyboardInterrupt\u001b[0m: \u001b[0m\u001b[0m\n",
      "\u001b[0mRealTimeSTT: realtimestt - ERROR - Unhandled exeption in _recording_worker: [Errno 9] Bad file descriptor\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/RealtimeSTT/audio_recorder.py\", line 1919, in _recording_worker\n",
      "    data = self.audio_queue.get(timeout=0.01)\n",
      "  File \"/home/irlab/.pyenv/versions/3.10.13/lib/python3.10/multiprocessing/queues.py\", line 117, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/home/irlab/.pyenv/versions/3.10.13/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/irlab/.pyenv/versions/3.10.13/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/irlab/.pyenv/versions/3.10.13/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python model.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdf7c75-2634-4d59-b47c-78f73edb3746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3743ad32-e1d1-4098-8fdb-78e87273f108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
