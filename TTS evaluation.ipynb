{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2fd974-46e9-475d-b7ba-9bbf7054b32e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flash attention 2 is not installed\n",
      "ParlerTTSForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "ParlerTTSForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n",
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n",
      "`prompt_attention_mask` is specified but `attention_mask` is not. A full `attention_mask` will be created. Make sure this is the intended behaviour.\n",
      "`prompt_attention_mask` is specified but `attention_mask` is not. A full `attention_mask` will be created. Make sure this is the intended behaviour.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔊 Playing chunk 1: 'भारत एक वि...' | Latency: 1.70s\n",
      "\n",
      "🔊 Playing chunk 2: 'शाल देश है...' | Latency: 3.42s\n",
      "\n",
      "🔊 Playing chunk 3: 'जो विभिन्न संस्कृति...' | Latency: 4.85s\n",
      "\n",
      "🔊 Playing chunk 4: 'यों, परंपराओं, भाषाओ...' | Latency: 6.78s\n",
      "\n",
      "🔊 Playing chunk 5: 'ं और प्राकृतिक सौंदर्य से भरा ...' | Latency: 8.91s\n",
      "\n",
      "🔊 Playing chunk 6: 'ाँ की विविधता इसकी पहचान है और...' | Latency: 12.46s\n",
      "\n",
      "🔊 Playing chunk 7: 'विश्व स्तर पर विशिष्ट बनाती है...' | Latency: 16.59s\n",
      "\n",
      "🔊 Playing chunk 8: 'िण में हिंद महासागर इसे घेरता ...' | Latency: 23.24s\n",
      "\n",
      "🔊 Playing chunk 9: 'तकनीकी युग तक फैला हुआ है। इसक...' | Latency: 29.87s\n",
      " Playback complete.\n",
      "\n",
      " Per-Chunk Latency Report:\n",
      "Chunk 1: Latency = 1.70s | 'भारत एक वि...'\n",
      "Chunk 2: Latency = 3.42s | 'शाल देश है...'\n",
      "Chunk 3: Latency = 4.85s | 'जो विभिन्न संस्कृति...'\n",
      "Chunk 4: Latency = 6.78s | 'यों, परंपराओं, भाषाओ...'\n",
      "Chunk 5: Latency = 8.91s | 'ं और प्राकृतिक सौंदर्य से भरा हुआ है। यह...'\n",
      "Chunk 6: Latency = 12.46s | 'ाँ की विविधता इसकी पहचान है और यह देश को...'\n",
      "Chunk 7: Latency = 16.59s | 'विश्व स्तर पर विशिष्ट बनाती है। यहां उत्तर में हिम...'\n",
      "Chunk 8: Latency = 23.24s | 'िण में हिंद महासागर इसे घेरता है। भारत का इतिहास प...'\n",
      "Chunk 9: Latency = 29.87s | 'तकनीकी युग तक फैला हुआ है। इसकी संस्कृति विभिन्न त...'\n",
      "\n",
      " Total Streaming Time: 38.81s\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import asyncio\n",
    "import queue\n",
    "import time\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "\n",
    "# === Device & Model Setup ===\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"ai4bharat/indic-parler-tts\")\n",
    "MODEL_1 = ParlerTTSForConditionalGeneration.from_pretrained(\"ai4bharat/indic-parler-tts\").to(DEVICE)\n",
    "MODEL_2 = ParlerTTSForConditionalGeneration.from_pretrained(\"ai4bharat/indic-parler-tts\").to(DEVICE)\n",
    "\n",
    "DESC_TOKENIZER = AutoTokenizer.from_pretrained(MODEL_1.config.text_encoder._name_or_path)\n",
    "DESCRIPTION = \"Rohit's voice is monotone yet slightly fast in delivery, with minimal background noise.\"\n",
    "DESC_INPUTS = DESC_TOKENIZER(DESCRIPTION, return_tensors=\"pt\").to(DEVICE)\n",
    "DESC_INPUT_IDS = DESC_INPUTS.input_ids\n",
    "DESC_ATTN_MASK = DESC_INPUTS.attention_mask\n",
    "\n",
    "SAMPLING_RATE = MODEL_1.config.sampling_rate\n",
    "sd.default.latency = 'low'\n",
    "\n",
    "# === TTS Worker ===\n",
    "def tts_worker(text_queue, audio_dict, model, audio_event_dict):\n",
    "    while True:\n",
    "        item = text_queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "\n",
    "        seq_id, text, start_time = item\n",
    "\n",
    "        prompt_inputs = TOKENIZER(text, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            audio_tensor = model.generate(\n",
    "                input_ids=DESC_INPUT_IDS,\n",
    "                attention_mask=DESC_ATTN_MASK,\n",
    "                prompt_input_ids=prompt_inputs.input_ids,\n",
    "                prompt_attention_mask=prompt_inputs.attention_mask\n",
    "            )\n",
    "\n",
    "        audio = audio_tensor.cpu().numpy().squeeze()\n",
    "        if audio.ndim > 1:\n",
    "            audio = audio.flatten()\n",
    "        audio = audio.astype(np.float32)\n",
    "        audio /= np.max(np.abs(audio)) if np.max(np.abs(audio)) else 1.0\n",
    "\n",
    "        audio_dict[seq_id] = (audio, text, start_time)\n",
    "        audio_event_dict[seq_id].set()\n",
    "\n",
    "# === Ordered Playback ===\n",
    "def ordered_playback(audio_dict, audio_event_dict, total_chunks, latency_records):\n",
    "    for seq_id in range(total_chunks):\n",
    "        audio_event_dict[seq_id].wait()\n",
    "        audio, text, start_time = audio_dict[seq_id]\n",
    "\n",
    "        latency = time.time() - start_time\n",
    "        latency_records.append((text, latency))\n",
    "        print(f\"\\n🔊 Playing chunk {seq_id+1}: '{text[:30]}...' | Latency: {latency:.2f}s\")\n",
    "\n",
    "        sd.play(audio, samplerate=SAMPLING_RATE)\n",
    "        sd.wait()\n",
    "\n",
    "    print(\" Playback complete.\")\n",
    "\n",
    "# === Dynamic Chunking ===\n",
    "def dynamic_chunking(text, initial_size, max_size):\n",
    "    chunks, toggle, sizes = [], 0, [initial_size, initial_size]\n",
    "    i = 0\n",
    "\n",
    "    while i < len(text):\n",
    "        chunk_size = sizes[toggle % 2]\n",
    "        chunk = text[i:i+chunk_size].strip()\n",
    "        chunks.append(chunk)\n",
    "        i += chunk_size\n",
    "\n",
    "        # Double chunk size on each toggle until reaching max_size\n",
    "        sizes[toggle % 2] = min(sizes[toggle % 2] * 2, max_size)\n",
    "        toggle += 1\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# === Main Function ===\n",
    "def parallel_ordered_tts_dynamic_chunks(prompt, initial_chunk_size=50, max_chunk_size=1000):\n",
    "    text_chunks = dynamic_chunking(prompt, initial_chunk_size, max_chunk_size)\n",
    "    total_chunks = len(text_chunks)\n",
    "\n",
    "    text_queue1, text_queue2 = queue.Queue(), queue.Queue()\n",
    "    audio_dict = {}\n",
    "    audio_event_dict = {i: threading.Event() for i in range(total_chunks)}\n",
    "    latency_records = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    for seq_id, chunk in enumerate(text_chunks):\n",
    "        target_queue = text_queue1 if seq_id % 2 == 0 else text_queue2\n",
    "        target_queue.put((seq_id, chunk, time.time()))\n",
    "\n",
    "    text_queue1.put(None)\n",
    "    text_queue2.put(None)\n",
    "\n",
    "    # TTS Workers\n",
    "    t1 = threading.Thread(target=tts_worker, args=(text_queue1, audio_dict, MODEL_1, audio_event_dict))\n",
    "    t2 = threading.Thread(target=tts_worker, args=(text_queue2, audio_dict, MODEL_2, audio_event_dict))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "\n",
    "    # Ordered Playback\n",
    "    ordered_playback(audio_dict, audio_event_dict, total_chunks, latency_records)\n",
    "\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "\n",
    "    total_duration = time.time() - start_time\n",
    "\n",
    "    # Latency report\n",
    "    print(\"\\n Per-Chunk Latency Report:\")\n",
    "    for idx, (chunk, latency) in enumerate(latency_records, 1):\n",
    "        print(f\"Chunk {idx}: Latency = {latency:.2f}s | '{chunk[:50]}...'\")\n",
    "\n",
    "    print(f\"\\n Total Streaming Time: {total_duration:.2f}s\")\n",
    "\n",
    "# === Example Usage ===\n",
    "if __name__ == \"__main__\":\n",
    "    hindi_text = (\n",
    "        \"भारत एक विशाल देश है जो विभिन्न संस्कृतियों, परंपराओं, भाषाओं और प्राकृतिक सौंदर्य से भरा हुआ है। \"\n",
    "        \"यहाँ की विविधता इसकी पहचान है और यह देश को विश्व स्तर पर विशिष्ट बनाती है। \"\n",
    "        \"यहां उत्तर में हिमालय पर्वत श्रृंखला है और दक्षिण में हिंद महासागर इसे घेरता है। \"\n",
    "        \"भारत का इतिहास प्राचीन सभ्यताओं से लेकर आधुनिक तकनीकी युग तक फैला हुआ है। \"\n",
    "        \"इसकी संस्कृति विभिन्न त्यौहारों, कला, संगीत, नृत्य और भोजन में झलकती है।\"\n",
    "    )\n",
    "\n",
    "    parallel_ordered_tts_dynamic_chunks(\n",
    "        hindi_text, \n",
    "        initial_chunk_size=10, \n",
    "        max_chunk_size=1000\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8049115-e8d7-45e7-81f9-15e9ed2c1ef7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flash attention 2 is not installed\n",
      "ParlerTTSForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "ParlerTTSForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m TOKENIZER \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mai4bharat/indic-parler-tts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m MODEL_1 \u001b[38;5;241m=\u001b[39m \u001b[43mParlerTTSForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mai4bharat/indic-parler-tts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m MODEL_2 \u001b[38;5;241m=\u001b[39m ParlerTTSForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mai4bharat/indic-parler-tts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     17\u001b[0m DESC_TOKENIZER \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_1\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtext_encoder\u001b[38;5;241m.\u001b[39m_name_or_path)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/pyvoice/lib/python3.10/site-packages/transformers/modeling_utils.py:3698\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3694\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3695\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3696\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3697\u001b[0m         )\n\u001b[0;32m-> 3698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 915 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import queue\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "from scipy.io.wavfile import write as wav_write\n",
    "\n",
    "# === Device & Model Setup ===\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"ai4bharat/indic-parler-tts\")\n",
    "MODEL_1 = ParlerTTSForConditionalGeneration.from_pretrained(\"ai4bharat/indic-parler-tts\").to(DEVICE)\n",
    "MODEL_2 = ParlerTTSForConditionalGeneration.from_pretrained(\"ai4bharat/indic-parler-tts\").to(DEVICE)\n",
    "\n",
    "DESC_TOKENIZER = AutoTokenizer.from_pretrained(MODEL_1.config.text_encoder._name_or_path)\n",
    "DESCRIPTION = \"Rohit's voice is monotone yet slightly fast in delivery, with minimal background noise.\"\n",
    "DESC_INPUTS = DESC_TOKENIZER(DESCRIPTION, return_tensors=\"pt\").to(DEVICE)\n",
    "DESC_INPUT_IDS = DESC_INPUTS.input_ids\n",
    "DESC_ATTN_MASK = DESC_INPUTS.attention_mask\n",
    "\n",
    "SAMPLING_RATE = MODEL_1.config.sampling_rate\n",
    "\n",
    "# === TTS Worker for WAV Generation ===\n",
    "def tts_worker(text_queue, audio_dict, model, audio_event_dict):\n",
    "    while True:\n",
    "        item = text_queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "\n",
    "        seq_id, text = item\n",
    "\n",
    "        prompt_inputs = TOKENIZER(text, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            audio_tensor = model.generate(\n",
    "                input_ids=DESC_INPUT_IDS,\n",
    "                attention_mask=DESC_ATTN_MASK,\n",
    "                prompt_input_ids=prompt_inputs.input_ids,\n",
    "                prompt_attention_mask=prompt_inputs.attention_mask\n",
    "            )\n",
    "\n",
    "        audio = audio_tensor.cpu().numpy().squeeze()\n",
    "        if audio.ndim > 1:\n",
    "            audio = audio.flatten()\n",
    "        audio = audio.astype(np.float32)\n",
    "        audio /= np.max(np.abs(audio)) if np.max(np.abs(audio)) else 1.0\n",
    "\n",
    "        audio_dict[seq_id] = audio\n",
    "        audio_event_dict[seq_id].set()\n",
    "\n",
    "# === Dynamic Chunking ===\n",
    "def dynamic_chunking(text, initial_size, max_size):\n",
    "    chunks, toggle, sizes = [], 0, [initial_size, initial_size]\n",
    "    i = 0\n",
    "\n",
    "    while i < len(text):\n",
    "        chunk_size = sizes[toggle % 2]\n",
    "        chunk = text[i:i+chunk_size].strip()\n",
    "        chunks.append(chunk)\n",
    "        i += chunk_size\n",
    "\n",
    "        # Double chunk size on each toggle until reaching max_size\n",
    "        sizes[toggle % 2] = min(sizes[toggle % 2] * 2, max_size)\n",
    "        toggle += 1\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# === Main Function for Audio Generation ===\n",
    "def generate_wav_from_text(prompt, initial_chunk_size=50, max_chunk_size=1000, output_file=\"generated_audio.wav\"):\n",
    "    text_chunks = dynamic_chunking(prompt, initial_chunk_size, max_chunk_size)\n",
    "    total_chunks = len(text_chunks)\n",
    "\n",
    "    text_queue1, text_queue2 = queue.Queue(), queue.Queue()\n",
    "    audio_dict = {}\n",
    "    audio_event_dict = {i: threading.Event() for i in range(total_chunks)}\n",
    "\n",
    "    # Distribute text chunks\n",
    "    for seq_id, chunk in enumerate(text_chunks):\n",
    "        target_queue = text_queue1 if seq_id % 2 == 0 else text_queue2\n",
    "        target_queue.put((seq_id, chunk))\n",
    "\n",
    "    text_queue1.put(None)\n",
    "    text_queue2.put(None)\n",
    "\n",
    "    # TTS worker threads\n",
    "    t1 = threading.Thread(target=tts_worker, args=(text_queue1, audio_dict, MODEL_1, audio_event_dict))\n",
    "    t2 = threading.Thread(target=tts_worker, args=(text_queue2, audio_dict, MODEL_2, audio_event_dict))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "\n",
    "    # Wait for TTS generation\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "\n",
    "    # Ensure audio is in correct order\n",
    "    audio_chunks = []\n",
    "    for seq_id in range(total_chunks):\n",
    "        audio_event_dict[seq_id].wait()\n",
    "        audio_chunks.append(audio_dict[seq_id])\n",
    "\n",
    "    # Concatenate audio chunks\n",
    "    combined_audio = np.concatenate(audio_chunks)\n",
    "\n",
    "    # Normalize audio\n",
    "    combined_audio /= np.max(np.abs(combined_audio)) if np.max(np.abs(combined_audio)) else 1.0\n",
    "\n",
    "    # Save audio as WAV file\n",
    "    wav_write(output_file, SAMPLING_RATE, (combined_audio * 32767).astype(np.int16))\n",
    "\n",
    "    print(f\"Audio successfully saved as '{output_file}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5bf18b5-7f7a-46ad-b8a2-ffbd9cffad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Audio successfully saved as 'generated_audio_2_50.wav'\n"
     ]
    }
   ],
   "source": [
    "hindi_text = (\"\"\"आधुनिक सूचना तकनीक के युग में कृत्रिम बुद्धिमत्ता यानी आर्टिफिशियल इंटेलिजेंस ने मानव जीवन में क्रांतिकारी परिवर्तन ला दिया है।\n",
    "मशीन लर्निंग, जो कृत्रिम बुद्धिमत्ता की एक उप-शाखा है, डाटा के माध्यम से स्वतः ही सीखने और भविष्यवाणी करने की क्षमता रखती है।\n",
    "न्यूरल नेटवर्क और डीप लर्निंग तकनीकें मानव मस्तिष्क की संरचना से प्रेरित हैं, जिनमें कृत्रिम न्यूरॉन आपस में जटिल गणितीय प्रक्रियाओं के माध्यम से संवाद करते हुए निर्णय लेते हैं।\"\"\"\n",
    "     \n",
    ")\n",
    "\n",
    "initial_chunk_size = 50  # Initial chunk size\n",
    "max_chunk_size = 1000    # Max chunk size\n",
    "\n",
    "generate_wav_from_text(\n",
    "    prompt=hindi_text,\n",
    "    initial_chunk_size=initial_chunk_size,\n",
    "    max_chunk_size=max_chunk_size,\n",
    "    output_file=f\"generated_audio_2_{initial_chunk_size}.wav\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cf3ce3a-26a5-41fc-8ba5-2f94e7fa7d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Audio successfully saved as 'generated_audio_3_10.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_3_20.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_3_30.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_3_40.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_3_50.wav'\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,60,10):\n",
    "    hindi_text = (\"\"\"इन तकनीकों का उपयोग कंप्यूटर विज़न, प्राकृतिक भाषा संसाधन और स्वचालित वाहन प्रणाली जैसे क्षेत्रों में व्यापक रूप से किया जा रहा है। हाल ही में विकसित किए गए चैटबॉट्स और जनरेटिव प्री-ट्रेन्ड ट्रांसफॉर्मर मॉडल, जिन्हें GPT कहा जाता है, मानव भाषाओं को समझने तथा उनके संदर्भ के अनुरूप अर्थपूर्ण जवाब उत्पन्न करने में सक्षम हैं। यद्यपि यह तकनीकें अत्यंत लाभकारी हैं, फिर भी डेटा की गोपनीयता, नैतिकता, तथा रोजगार पर इनका प्रभाव जैसे जटिल प्रश्न भी उत्पन्न होते हैं, जिनका समाधान आने वाले समय में मानव और मशीन के मध्य संबंधों की दिशा निर्धारित करेगा।\"\"\")\n",
    "\n",
    "    initial_chunk_size = i  # Initial chunk size\n",
    "    max_chunk_size = 1000    # Max chunk size\n",
    "\n",
    "    generate_wav_from_text(\n",
    "    prompt=hindi_text,\n",
    "    initial_chunk_size=initial_chunk_size,\n",
    "    max_chunk_size=max_chunk_size,\n",
    "    output_file=f\"generated_audio_3_{initial_chunk_size}.wav\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e45e14ee-cefd-47ee-9262-5f3a0a7d5456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Audio successfully saved as 'generated_audio_4_10.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_4_20.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_4_30.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_4_40.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_4_50.wav'\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,60,10):\n",
    "    hindi_text = (\"\"\"मानव शरीर एक अत्यंत जटिल तंत्र है जिसमें विभिन्न अंग आपस में सामंजस्य के साथ कार्य करते हैं। हमारा हृदय रक्त परिसंचरण प्रणाली का केंद्र है, जो पूरे शरीर में ऑक्सीजन और पोषक तत्वों की आपूर्ति सुनिश्चित करता है। रक्त वाहिकाओं का जाल धमनियों और शिराओं के माध्यम से रक्त को हर अंग तक पहुँचाता है। इसके अलावा, तंत्रिका तंत्र शरीर के हर हिस्से को नियंत्रित करता है, जिसमें मस्तिष्क, रीढ़ की हड्डी और परिधीय तंत्रिकाएँ प्रमुख भूमिका निभाती हैं। \"\"\")\n",
    "\n",
    "    initial_chunk_size = i  # Initial chunk size\n",
    "    max_chunk_size = 1000    # Max chunk size\n",
    "\n",
    "    generate_wav_from_text(\n",
    "    prompt=hindi_text,\n",
    "    initial_chunk_size=initial_chunk_size,\n",
    "    max_chunk_size=max_chunk_size,\n",
    "    output_file=f\"generated_audio_4_{initial_chunk_size}.wav\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "211b1a4a-7813-434e-b98d-e008ae44a99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Audio successfully saved as 'generated_audio_5_10.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_5_20.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_5_30.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_5_40.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_5_50.wav'\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,60,10):\n",
    "    hindi_text = ('''एक समय की बात है, दूर किसी गाँव में एक बूढ़ा किसान अपने बेटे के साथ रहता था। उनके पास एक घोड़ा था, जो गाँव भर में अपनी खूबसूरती और ताकत के लिए प्रसिद्ध था। एक दिन वह घोड़ा अचानक लापता हो गया। गाँव के लोग यह खबर सुनकर किसान के पास आए और बोले, \"तुम्हारे साथ बड़ी दुर्भाग्यपूर्ण घटना घटी है।\" किसान ने सहजता से जवाब दिया, \"शायद ये अच्छा है या बुरा, कौन जाने?\" कुछ दिनों बाद, किसान का घोड़ा वापस आया, और अपने साथ जंगली घोड़ों का एक पूरा झुंड ले आया। गाँव के लोग फिर किसान के पास पहुँचे और बोले, \"तुम्हारा भाग्य बहुत अच्छा है। एक घोड़े के बदले अब तुम्हारे पास कई घोड़े आ गए हैं।\" किसान फिर मुस्कुराते हुए बोला, \"शायद अच्छा है, शायद नहीं, कौन जाने?''')\n",
    "\n",
    "    initial_chunk_size = i  # Initial chunk size\n",
    "    max_chunk_size = 1000    # Max chunk size\n",
    "\n",
    "    generate_wav_from_text(\n",
    "    prompt=hindi_text,\n",
    "    initial_chunk_size=initial_chunk_size,\n",
    "    max_chunk_size=max_chunk_size,\n",
    "    output_file=f\"generated_audio_5_{initial_chunk_size}.wav\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00d022f5-de8a-4396-89b0-5890e8f5dd8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`prompt_attention_mask` is specified but `attention_mask` is not. A full `attention_mask` will be created. Make sure this is the intended behaviour.\n",
      "`prompt_attention_mask` is specified but `attention_mask` is not. A full `attention_mask` will be created. Make sure this is the intended behaviour.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Audio successfully saved as 'generated_audio_6_40.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_6_50.wav'\n"
     ]
    }
   ],
   "source": [
    "for i in range(40,60,10):\n",
    "    hindi_text = ('''कुछ दिन बीते, और किसान का बेटा उन नए जंगली घोड़ों को प्रशिक्षित करते समय गिर पड़ा और उसकी टाँग टूट गई। फिर से लोग सहानुभूति जताने आए और बोले, \"ये तो बहुत बुरा हुआ, अब तुम्हारा बेटा खेती-बाड़ी में मदद नहीं कर पाएगा।\" किसान ने उसी शांति के साथ उत्तर दिया, \"अच्छा है या बुरा, ये तो समय बताएगा।\" अचानक गाँव में युद्ध की घोषणा हो गई। राजा के सिपाही गाँव-गाँव जाकर युवकों को सेना में भर्ती करने लगे। किसान के बेटे की टूटी टाँग देखकर उन्होंने उसे भर्ती नहीं किया। गाँव के युवक सेना में गए, जिनमें से कई युद्ध से वापस नहीं लौटे। गाँव के लोग फिर किसान के पास आए और बोले, \"तुम्हारी दूरदर्शिता सच साबित हुई। तुम्हारा बेटा सुरक्षित है।\" किसान ने फिर उसी शांत मुस्कान के साथ कहा, \"अच्छा हुआ या बुरा, इसे सिर्फ़ समय ही बता सकता है।\" यह कहानी हमें सिखाती है कि जीवन की हर परिस्थिति में तुरंत फैसला न करें। क्योंकि जो घटना आज हमें बुरी लगती है, शायद वही कल हमारे लिए अच्छी साबित हो जाए।''')\n",
    "\n",
    "    initial_chunk_size = i  # Initial chunk size\n",
    "    max_chunk_size = 1000    # Max chunk size\n",
    "\n",
    "    generate_wav_from_text(\n",
    "    prompt=hindi_text,\n",
    "    initial_chunk_size=initial_chunk_size,\n",
    "    max_chunk_size=max_chunk_size,\n",
    "    output_file=f\"generated_audio_6_{initial_chunk_size}.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e12242e-e8e4-453f-bbae-e724cf057b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Audio successfully saved as 'generated_audio_7_10.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_7_20.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_7_30.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_7_40.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_7_50.wav'\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,60,10):\n",
    "    hindi_text = ('''मुझे पानी पीना है। क्या आप मेरी मदद कर सकते हैं? आज मौसम बहुत अच्छा है। यह किताब मुझे बहुत पसंद है।''')\n",
    "\n",
    "    initial_chunk_size = i  # Initial chunk size\n",
    "    max_chunk_size = 1000    # Max chunk size\n",
    "\n",
    "    generate_wav_from_text(\n",
    "    prompt=hindi_text,\n",
    "    initial_chunk_size=initial_chunk_size,\n",
    "    max_chunk_size=max_chunk_size,\n",
    "    output_file=f\"generated_audio_7_{initial_chunk_size}.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b27ec5-25c9-4f01-8649-e31da5eea60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Audio successfully saved as 'generated_audio_8_10.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_8_20.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_8_30.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_8_40.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_8_50.wav'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(10,60,10):\n",
    "    hindi_text = ('''चलो पार्क में चलते हैं। मुझे थोड़ी देर सोना है। तुम कहाँ जा रहे हो?''')\n",
    "\n",
    "    initial_chunk_size = i  # Initial chunk size\n",
    "    max_chunk_size = 1000    # Max chunk size\n",
    "\n",
    "    generate_wav_from_text(\n",
    "    prompt=hindi_text,\n",
    "    initial_chunk_size=initial_chunk_size,\n",
    "    max_chunk_size=max_chunk_size,\n",
    "    output_file=f\"generated_audio_8_{initial_chunk_size}.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "697d792a-2ff7-4e7d-ab7c-056cdd2eb98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Audio successfully saved as 'generated_audio_9_10.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_9_20.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_9_30.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_9_40.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_9_50.wav'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(10,60,10):\n",
    "    hindi_text = ('''वह बहुत समझदार बच्चा है। यह काम अब पूरा हो गया है। धन्यवाद, आपने समय निकाला।''')\n",
    "\n",
    "    initial_chunk_size = i  # Initial chunk size\n",
    "    max_chunk_size = 1000    # Max chunk size\n",
    "\n",
    "    generate_wav_from_text(\n",
    "    prompt=hindi_text,\n",
    "    initial_chunk_size=initial_chunk_size,\n",
    "    max_chunk_size=max_chunk_size,\n",
    "    output_file=f\"generated_audio_9_{initial_chunk_size}.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db70c4c3-f895-466d-861b-a1479c4c5495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Audio successfully saved as 'generated_audio_10_10.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_10_20.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_10_30.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_10_40.wav'\n",
      "✅ Audio successfully saved as 'generated_audio_10_50.wav'\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,60,10):\n",
    "    hindi_text = ('''यह तो बहुत अच्छी खबर है! मुझे आज बहुत खुशी हो रही है। मुझे अकेलापन महसूस हो रहा है। आज का दिन कुछ अच्छा नहीं गया। तुमने मेरी बात क्यों नहीं मानी? यह बिल्कुल भी ठीक नहीं हुआ।''')\n",
    "\n",
    "    initial_chunk_size = i  # Initial chunk size\n",
    "    max_chunk_size = 1000    # Max chunk size\n",
    "\n",
    "    generate_wav_from_text(\n",
    "    prompt=hindi_text,\n",
    "    initial_chunk_size=initial_chunk_size,\n",
    "    max_chunk_size=max_chunk_size,\n",
    "    output_file=f\"generated_audio_10_{initial_chunk_size}.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1290c03-9b70-4174-9fd0-343a8e502401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_path  ... predict_dataset\n",
      "0  /media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254...  ...         sarulab\n",
      "1  /media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254...  ...         sarulab\n",
      "2  /media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254...  ...         sarulab\n",
      "3  /media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254...  ...         sarulab\n",
      "4  /media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254...  ...         sarulab\n",
      "\n",
      "[5 rows x 4 columns]\n",
      "Using model: ssl_multispec_ext_v2\n",
      "/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Loaded weight from /home/irlab/.cache/utmosv2/models/fusion_stage3/fold0_s42_best_model.pth\n",
      "+*+*[[Fold 1/5]]+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n",
      "  [Inference] (1/1):   0%|                                | 0/7 [00:00<?, ?it/s]/media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254e15/Parth/Voice/UTMOSv2/utmosv2/runner/_inference.py:60: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "  [Inference] (1/1): 100%|████████████████████████| 7/7 [00:31<00:00,  4.46s/it]\n",
      "Average of 1 folds\n",
      "Predictions are saved to /media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254e15/Parth/Voice/gen_audios/result.csv\n"
     ]
    }
   ],
   "source": [
    "!python ./UTMOSv2/inference.py --input_dir /media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254e15/Parth/Voice/gen_audios --out_path /media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254e15/Parth/Voice/gen_audios/result.csv\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
