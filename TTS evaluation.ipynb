{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2fd974-46e9-475d-b7ba-9bbf7054b32e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flash attention 2 is not installed\n",
      "ParlerTTSForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "ParlerTTSForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n",
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n",
      "`prompt_attention_mask` is specified but `attention_mask` is not. A full `attention_mask` will be created. Make sure this is the intended behaviour.\n",
      "`prompt_attention_mask` is specified but `attention_mask` is not. A full `attention_mask` will be created. Make sure this is the intended behaviour.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Š Playing chunk 1: 'à¤­à¤¾à¤°à¤¤ à¤à¤• à¤µà¤¿...' | Latency: 1.70s\n",
      "\n",
      "ðŸ”Š Playing chunk 2: 'à¤¶à¤¾à¤² à¤¦à¥‡à¤¶ à¤¹à¥ˆ...' | Latency: 3.42s\n",
      "\n",
      "ðŸ”Š Playing chunk 3: 'à¤œà¥‹ à¤µà¤¿à¤­à¤¿à¤¨à¥à¤¨ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿...' | Latency: 4.85s\n",
      "\n",
      "ðŸ”Š Playing chunk 4: 'à¤¯à¥‹à¤‚, à¤ªà¤°à¤‚à¤ªà¤°à¤¾à¤“à¤‚, à¤­à¤¾à¤·à¤¾à¤“...' | Latency: 6.78s\n",
      "\n",
      "ðŸ”Š Playing chunk 5: 'à¤‚ à¤”à¤° à¤ªà¥à¤°à¤¾à¤•à¥ƒà¤¤à¤¿à¤• à¤¸à¥Œà¤‚à¤¦à¤°à¥à¤¯ à¤¸à¥‡ à¤­à¤°à¤¾ ...' | Latency: 8.91s\n",
      "\n",
      "ðŸ”Š Playing chunk 6: 'à¤¾à¤ à¤•à¥€ à¤µà¤¿à¤µà¤¿à¤§à¤¤à¤¾ à¤‡à¤¸à¤•à¥€ à¤ªà¤¹à¤šà¤¾à¤¨ à¤¹à¥ˆ à¤”à¤°...' | Latency: 12.46s\n",
      "\n",
      "ðŸ”Š Playing chunk 7: 'à¤µà¤¿à¤¶à¥à¤µ à¤¸à¥à¤¤à¤° à¤ªà¤° à¤µà¤¿à¤¶à¤¿à¤·à¥à¤Ÿ à¤¬à¤¨à¤¾à¤¤à¥€ à¤¹à¥ˆ...' | Latency: 16.59s\n",
      "\n",
      "ðŸ”Š Playing chunk 8: 'à¤¿à¤£ à¤®à¥‡à¤‚ à¤¹à¤¿à¤‚à¤¦ à¤®à¤¹à¤¾à¤¸à¤¾à¤—à¤° à¤‡à¤¸à¥‡ à¤˜à¥‡à¤°à¤¤à¤¾ ...' | Latency: 23.24s\n",
      "\n",
      "ðŸ”Š Playing chunk 9: 'à¤¤à¤•à¤¨à¥€à¤•à¥€ à¤¯à¥à¤— à¤¤à¤• à¤«à¥ˆà¤²à¤¾ à¤¹à¥à¤† à¤¹à¥ˆà¥¤ à¤‡à¤¸à¤•...' | Latency: 29.87s\n",
      " Playback complete.\n",
      "\n",
      " Per-Chunk Latency Report:\n",
      "Chunk 1: Latency = 1.70s | 'à¤­à¤¾à¤°à¤¤ à¤à¤• à¤µà¤¿...'\n",
      "Chunk 2: Latency = 3.42s | 'à¤¶à¤¾à¤² à¤¦à¥‡à¤¶ à¤¹à¥ˆ...'\n",
      "Chunk 3: Latency = 4.85s | 'à¤œà¥‹ à¤µà¤¿à¤­à¤¿à¤¨à¥à¤¨ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿...'\n",
      "Chunk 4: Latency = 6.78s | 'à¤¯à¥‹à¤‚, à¤ªà¤°à¤‚à¤ªà¤°à¤¾à¤“à¤‚, à¤­à¤¾à¤·à¤¾à¤“...'\n",
      "Chunk 5: Latency = 8.91s | 'à¤‚ à¤”à¤° à¤ªà¥à¤°à¤¾à¤•à¥ƒà¤¤à¤¿à¤• à¤¸à¥Œà¤‚à¤¦à¤°à¥à¤¯ à¤¸à¥‡ à¤­à¤°à¤¾ à¤¹à¥à¤† à¤¹à¥ˆà¥¤ à¤¯à¤¹...'\n",
      "Chunk 6: Latency = 12.46s | 'à¤¾à¤ à¤•à¥€ à¤µà¤¿à¤µà¤¿à¤§à¤¤à¤¾ à¤‡à¤¸à¤•à¥€ à¤ªà¤¹à¤šà¤¾à¤¨ à¤¹à¥ˆ à¤”à¤° à¤¯à¤¹ à¤¦à¥‡à¤¶ à¤•à¥‹...'\n",
      "Chunk 7: Latency = 16.59s | 'à¤µà¤¿à¤¶à¥à¤µ à¤¸à¥à¤¤à¤° à¤ªà¤° à¤µà¤¿à¤¶à¤¿à¤·à¥à¤Ÿ à¤¬à¤¨à¤¾à¤¤à¥€ à¤¹à¥ˆà¥¤ à¤¯à¤¹à¤¾à¤‚ à¤‰à¤¤à¥à¤¤à¤° à¤®à¥‡à¤‚ à¤¹à¤¿à¤®...'\n",
      "Chunk 8: Latency = 23.24s | 'à¤¿à¤£ à¤®à¥‡à¤‚ à¤¹à¤¿à¤‚à¤¦ à¤®à¤¹à¤¾à¤¸à¤¾à¤—à¤° à¤‡à¤¸à¥‡ à¤˜à¥‡à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤­à¤¾à¤°à¤¤ à¤•à¤¾ à¤‡à¤¤à¤¿à¤¹à¤¾à¤¸ à¤ª...'\n",
      "Chunk 9: Latency = 29.87s | 'à¤¤à¤•à¤¨à¥€à¤•à¥€ à¤¯à¥à¤— à¤¤à¤• à¤«à¥ˆà¤²à¤¾ à¤¹à¥à¤† à¤¹à¥ˆà¥¤ à¤‡à¤¸à¤•à¥€ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿ à¤µà¤¿à¤­à¤¿à¤¨à¥à¤¨ à¤¤...'\n",
      "\n",
      " Total Streaming Time: 38.81s\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import asyncio\n",
    "import queue\n",
    "import time\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "\n",
    "# === Device & Model Setup ===\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"ai4bharat/indic-parler-tts\")\n",
    "MODEL_1 = ParlerTTSForConditionalGeneration.from_pretrained(\"ai4bharat/indic-parler-tts\").to(DEVICE)\n",
    "MODEL_2 = ParlerTTSForConditionalGeneration.from_pretrained(\"ai4bharat/indic-parler-tts\").to(DEVICE)\n",
    "\n",
    "DESC_TOKENIZER = AutoTokenizer.from_pretrained(MODEL_1.config.text_encoder._name_or_path)\n",
    "DESCRIPTION = \"Rohit's voice is monotone yet slightly fast in delivery, with minimal background noise.\"\n",
    "DESC_INPUTS = DESC_TOKENIZER(DESCRIPTION, return_tensors=\"pt\").to(DEVICE)\n",
    "DESC_INPUT_IDS = DESC_INPUTS.input_ids\n",
    "DESC_ATTN_MASK = DESC_INPUTS.attention_mask\n",
    "\n",
    "SAMPLING_RATE = MODEL_1.config.sampling_rate\n",
    "sd.default.latency = 'low'\n",
    "\n",
    "# === TTS Worker ===\n",
    "def tts_worker(text_queue, audio_dict, model, audio_event_dict):\n",
    "    while True:\n",
    "        item = text_queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "\n",
    "        seq_id, text, start_time = item\n",
    "\n",
    "        prompt_inputs = TOKENIZER(text, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            audio_tensor = model.generate(\n",
    "                input_ids=DESC_INPUT_IDS,\n",
    "                attention_mask=DESC_ATTN_MASK,\n",
    "                prompt_input_ids=prompt_inputs.input_ids,\n",
    "                prompt_attention_mask=prompt_inputs.attention_mask\n",
    "            )\n",
    "\n",
    "        audio = audio_tensor.cpu().numpy().squeeze()\n",
    "        if audio.ndim > 1:\n",
    "            audio = audio.flatten()\n",
    "        audio = audio.astype(np.float32)\n",
    "        audio /= np.max(np.abs(audio)) if np.max(np.abs(audio)) else 1.0\n",
    "\n",
    "        audio_dict[seq_id] = (audio, text, start_time)\n",
    "        audio_event_dict[seq_id].set()\n",
    "\n",
    "# === Ordered Playback ===\n",
    "def ordered_playback(audio_dict, audio_event_dict, total_chunks, latency_records):\n",
    "    for seq_id in range(total_chunks):\n",
    "        audio_event_dict[seq_id].wait()\n",
    "        audio, text, start_time = audio_dict[seq_id]\n",
    "\n",
    "        latency = time.time() - start_time\n",
    "        latency_records.append((text, latency))\n",
    "        print(f\"\\nðŸ”Š Playing chunk {seq_id+1}: '{text[:30]}...' | Latency: {latency:.2f}s\")\n",
    "\n",
    "        sd.play(audio, samplerate=SAMPLING_RATE)\n",
    "        sd.wait()\n",
    "\n",
    "    print(\" Playback complete.\")\n",
    "\n",
    "# === Dynamic Chunking ===\n",
    "def dynamic_chunking(text, initial_size, max_size):\n",
    "    chunks, toggle, sizes = [], 0, [initial_size, initial_size]\n",
    "    i = 0\n",
    "\n",
    "    while i < len(text):\n",
    "        chunk_size = sizes[toggle % 2]\n",
    "        chunk = text[i:i+chunk_size].strip()\n",
    "        chunks.append(chunk)\n",
    "        i += chunk_size\n",
    "\n",
    "        # Double chunk size on each toggle until reaching max_size\n",
    "        sizes[toggle % 2] = min(sizes[toggle % 2] * 2, max_size)\n",
    "        toggle += 1\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# === Main Function ===\n",
    "def parallel_ordered_tts_dynamic_chunks(prompt, initial_chunk_size=50, max_chunk_size=1000):\n",
    "    text_chunks = dynamic_chunking(prompt, initial_chunk_size, max_chunk_size)\n",
    "    total_chunks = len(text_chunks)\n",
    "\n",
    "    text_queue1, text_queue2 = queue.Queue(), queue.Queue()\n",
    "    audio_dict = {}\n",
    "    audio_event_dict = {i: threading.Event() for i in range(total_chunks)}\n",
    "    latency_records = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    for seq_id, chunk in enumerate(text_chunks):\n",
    "        target_queue = text_queue1 if seq_id % 2 == 0 else text_queue2\n",
    "        target_queue.put((seq_id, chunk, time.time()))\n",
    "\n",
    "    text_queue1.put(None)\n",
    "    text_queue2.put(None)\n",
    "\n",
    "    # TTS Workers\n",
    "    t1 = threading.Thread(target=tts_worker, args=(text_queue1, audio_dict, MODEL_1, audio_event_dict))\n",
    "    t2 = threading.Thread(target=tts_worker, args=(text_queue2, audio_dict, MODEL_2, audio_event_dict))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "\n",
    "    # Ordered Playback\n",
    "    ordered_playback(audio_dict, audio_event_dict, total_chunks, latency_records)\n",
    "\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "\n",
    "    total_duration = time.time() - start_time\n",
    "\n",
    "    # Latency report\n",
    "    print(\"\\n Per-Chunk Latency Report:\")\n",
    "    for idx, (chunk, latency) in enumerate(latency_records, 1):\n",
    "        print(f\"Chunk {idx}: Latency = {latency:.2f}s | '{chunk[:50]}...'\")\n",
    "\n",
    "    print(f\"\\n Total Streaming Time: {total_duration:.2f}s\")\n",
    "\n",
    "# === Example Usage ===\n",
    "if __name__ == \"__main__\":\n",
    "    hindi_text = (\n",
    "        \"à¤­à¤¾à¤°à¤¤ à¤à¤• à¤µà¤¿à¤¶à¤¾à¤² à¤¦à¥‡à¤¶ à¤¹à¥ˆ à¤œà¥‹ à¤µà¤¿à¤­à¤¿à¤¨à¥à¤¨ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿à¤¯à¥‹à¤‚, à¤ªà¤°à¤‚à¤ªà¤°à¤¾à¤“à¤‚, à¤­à¤¾à¤·à¤¾à¤“à¤‚ à¤”à¤° à¤ªà¥à¤°à¤¾à¤•à¥ƒà¤¤à¤¿à¤• à¤¸à¥Œà¤‚à¤¦à¤°à¥à¤¯ à¤¸à¥‡ à¤­à¤°à¤¾ à¤¹à¥à¤† à¤¹à¥ˆà¥¤ \"\n",
    "        \"à¤¯à¤¹à¤¾à¤ à¤•à¥€ à¤µà¤¿à¤µà¤¿à¤§à¤¤à¤¾ à¤‡à¤¸à¤•à¥€ à¤ªà¤¹à¤šà¤¾à¤¨ à¤¹à¥ˆ à¤”à¤° à¤¯à¤¹ à¤¦à¥‡à¤¶ à¤•à¥‹ à¤µà¤¿à¤¶à¥à¤µ à¤¸à¥à¤¤à¤° à¤ªà¤° à¤µà¤¿à¤¶à¤¿à¤·à¥à¤Ÿ à¤¬à¤¨à¤¾à¤¤à¥€ à¤¹à¥ˆà¥¤ \"\n",
    "        \"à¤¯à¤¹à¤¾à¤‚ à¤‰à¤¤à¥à¤¤à¤° à¤®à¥‡à¤‚ à¤¹à¤¿à¤®à¤¾à¤²à¤¯ à¤ªà¤°à¥à¤µà¤¤ à¤¶à¥à¤°à¥ƒà¤‚à¤–à¤²à¤¾ à¤¹à¥ˆ à¤”à¤° à¤¦à¤•à¥à¤·à¤¿à¤£ à¤®à¥‡à¤‚ à¤¹à¤¿à¤‚à¤¦ à¤®à¤¹à¤¾à¤¸à¤¾à¤—à¤° à¤‡à¤¸à¥‡ à¤˜à¥‡à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤ \"\n",
    "        \"à¤­à¤¾à¤°à¤¤ à¤•à¤¾ à¤‡à¤¤à¤¿à¤¹à¤¾à¤¸ à¤ªà¥à¤°à¤¾à¤šà¥€à¤¨ à¤¸à¤­à¥à¤¯à¤¤à¤¾à¤“à¤‚ à¤¸à¥‡ à¤²à¥‡à¤•à¤° à¤†à¤§à¥à¤¨à¤¿à¤• à¤¤à¤•à¤¨à¥€à¤•à¥€ à¤¯à¥à¤— à¤¤à¤• à¤«à¥ˆà¤²à¤¾ à¤¹à¥à¤† à¤¹à¥ˆà¥¤ \"\n",
    "        \"à¤‡à¤¸à¤•à¥€ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿ à¤µà¤¿à¤­à¤¿à¤¨à¥à¤¨ à¤¤à¥à¤¯à¥Œà¤¹à¤¾à¤°à¥‹à¤‚, à¤•à¤²à¤¾, à¤¸à¤‚à¤—à¥€à¤¤, à¤¨à¥ƒà¤¤à¥à¤¯ à¤”à¤° à¤­à¥‹à¤œà¤¨ à¤®à¥‡à¤‚ à¤à¤²à¤•à¤¤à¥€ à¤¹à¥ˆà¥¤\"\n",
    "    )\n",
    "\n",
    "    parallel_ordered_tts_dynamic_chunks(\n",
    "        hindi_text, \n",
    "        initial_chunk_size=10, \n",
    "        max_chunk_size=1000\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8049115-e8d7-45e7-81f9-15e9ed2c1ef7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flash attention 2 is not installed\n",
      "ParlerTTSForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "ParlerTTSForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m TOKENIZER \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mai4bharat/indic-parler-tts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m MODEL_1 \u001b[38;5;241m=\u001b[39m \u001b[43mParlerTTSForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mai4bharat/indic-parler-tts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m MODEL_2 \u001b[38;5;241m=\u001b[39m ParlerTTSForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mai4bharat/indic-parler-tts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     17\u001b[0m DESC_TOKENIZER \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_1\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtext_encoder\u001b[38;5;241m.\u001b[39m_name_or_path)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/pyvoice/lib/python3.10/site-packages/transformers/modeling_utils.py:3698\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3694\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3695\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3696\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3697\u001b[0m         )\n\u001b[0;32m-> 3698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 915 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/pyvoice/lib/python3.10/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import queue\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "from scipy.io.wavfile import write as wav_write\n",
    "\n",
    "# === Device & Model Setup ===\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"ai4bharat/indic-parler-tts\")\n",
    "MODEL_1 = ParlerTTSForConditionalGeneration.from_pretrained(\"ai4bharat/indic-parler-tts\").to(DEVICE)\n",
    "MODEL_2 = ParlerTTSForConditionalGeneration.from_pretrained(\"ai4bharat/indic-parler-tts\").to(DEVICE)\n",
    "\n",
    "DESC_TOKENIZER = AutoTokenizer.from_pretrained(MODEL_1.config.text_encoder._name_or_path)\n",
    "DESCRIPTION = \"Rohit's voice is monotone yet slightly fast in delivery, with minimal background noise.\"\n",
    "DESC_INPUTS = DESC_TOKENIZER(DESCRIPTION, return_tensors=\"pt\").to(DEVICE)\n",
    "DESC_INPUT_IDS = DESC_INPUTS.input_ids\n",
    "DESC_ATTN_MASK = DESC_INPUTS.attention_mask\n",
    "\n",
    "SAMPLING_RATE = MODEL_1.config.sampling_rate\n",
    "\n",
    "# === TTS Worker for WAV Generation ===\n",
    "def tts_worker(text_queue, audio_dict, model, audio_event_dict):\n",
    "    while True:\n",
    "        item = text_queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "\n",
    "        seq_id, text = item\n",
    "\n",
    "        prompt_inputs = TOKENIZER(text, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            audio_tensor = model.generate(\n",
    "                input_ids=DESC_INPUT_IDS,\n",
    "                attention_mask=DESC_ATTN_MASK,\n",
    "                prompt_input_ids=prompt_inputs.input_ids,\n",
    "                prompt_attention_mask=prompt_inputs.attention_mask\n",
    "            )\n",
    "\n",
    "        audio = audio_tensor.cpu().numpy().squeeze()\n",
    "        if audio.ndim > 1:\n",
    "            audio = audio.flatten()\n",
    "        audio = audio.astype(np.float32)\n",
    "        audio /= np.max(np.abs(audio)) if np.max(np.abs(audio)) else 1.0\n",
    "\n",
    "        audio_dict[seq_id] = audio\n",
    "        audio_event_dict[seq_id].set()\n",
    "\n",
    "# === Dynamic Chunking ===\n",
    "def dynamic_chunking(text, initial_size, max_size):\n",
    "    chunks, toggle, sizes = [], 0, [initial_size, initial_size]\n",
    "    i = 0\n",
    "\n",
    "    while i < len(text):\n",
    "        chunk_size = sizes[toggle % 2]\n",
    "        chunk = text[i:i+chunk_size].strip()\n",
    "        chunks.append(chunk)\n",
    "        i += chunk_size\n",
    "\n",
    "        # Double chunk size on each toggle until reaching max_size\n",
    "        sizes[toggle % 2] = min(sizes[toggle % 2] * 2, max_size)\n",
    "        toggle += 1\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# === Main Function for Audio Generation ===\n",
    "def generate_wav_from_text(prompt, initial_chunk_size=50, max_chunk_size=1000, output_file=\"generated_audio.wav\"):\n",
    "    text_chunks = dynamic_chunking(prompt, initial_chunk_size, max_chunk_size)\n",
    "    total_chunks = len(text_chunks)\n",
    "\n",
    "    text_queue1, text_queue2 = queue.Queue(), queue.Queue()\n",
    "    audio_dict = {}\n",
    "    audio_event_dict = {i: threading.Event() for i in range(total_chunks)}\n",
    "\n",
    "    # Distribute text chunks\n",
    "    for seq_id, chunk in enumerate(text_chunks):\n",
    "        target_queue = text_queue1 if seq_id % 2 == 0 else text_queue2\n",
    "        target_queue.put((seq_id, chunk))\n",
    "\n",
    "    text_queue1.put(None)\n",
    "    text_queue2.put(None)\n",
    "\n",
    "    # TTS worker threads\n",
    "    t1 = threading.Thread(target=tts_worker, args=(text_queue1, audio_dict, MODEL_1, audio_event_dict))\n",
    "    t2 = threading.Thread(target=tts_worker, args=(text_queue2, audio_dict, MODEL_2, audio_event_dict))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "\n",
    "    # Wait for TTS generation\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "\n",
    "    # Ensure audio is in correct order\n",
    "    audio_chunks = []\n",
    "    for seq_id in range(total_chunks):\n",
    "        audio_event_dict[seq_id].wait()\n",
    "        audio_chunks.append(audio_dict[seq_id])\n",
    "\n",
    "    # Concatenate audio chunks\n",
    "    combined_audio = np.concatenate(audio_chunks)\n",
    "\n",
    "    # Normalize audio\n",
    "    combined_audio /= np.max(np.abs(combined_audio)) if np.max(np.abs(combined_audio)) else 1.0\n",
    "\n",
    "    # Save audio as WAV file\n",
    "    wav_write(output_file, SAMPLING_RATE, (combined_audio * 32767).astype(np.int16))\n",
    "\n",
    "    print(f\"Audio successfully saved as '{output_file}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5bf18b5-7f7a-46ad-b8a2-ffbd9cffad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Audio successfully saved as 'generated_audio_2_50.wav'\n"
     ]
    }
   ],
   "source": [
    "hindi_text = (\"\"\"à¤†à¤§à¥à¤¨à¤¿à¤• à¤¸à¥‚à¤šà¤¨à¤¾ à¤¤à¤•à¤¨à¥€à¤• à¤•à¥‡ à¤¯à¥à¤— à¤®à¥‡à¤‚ à¤•à¥ƒà¤¤à¥à¤°à¤¿à¤® à¤¬à¥à¤¦à¥à¤§à¤¿à¤®à¤¤à¥à¤¤à¤¾ à¤¯à¤¾à¤¨à¥€ à¤†à¤°à¥à¤Ÿà¤¿à¤«à¤¿à¤¶à¤¿à¤¯à¤² à¤‡à¤‚à¤Ÿà¥‡à¤²à¤¿à¤œà¥‡à¤‚à¤¸ à¤¨à¥‡ à¤®à¤¾à¤¨à¤µ à¤œà¥€à¤µà¤¨ à¤®à¥‡à¤‚ à¤•à¥à¤°à¤¾à¤‚à¤¤à¤¿à¤•à¤¾à¤°à¥€ à¤ªà¤°à¤¿à¤µà¤°à¥à¤¤à¤¨ à¤²à¤¾ à¤¦à¤¿à¤¯à¤¾ à¤¹à¥ˆà¥¤\n",
    "à¤®à¤¶à¥€à¤¨ à¤²à¤°à¥à¤¨à¤¿à¤‚à¤—, à¤œà¥‹ à¤•à¥ƒà¤¤à¥à¤°à¤¿à¤® à¤¬à¥à¤¦à¥à¤§à¤¿à¤®à¤¤à¥à¤¤à¤¾ à¤•à¥€ à¤à¤• à¤‰à¤ª-à¤¶à¤¾à¤–à¤¾ à¤¹à¥ˆ, à¤¡à¤¾à¤Ÿà¤¾ à¤•à¥‡ à¤®à¤¾à¤§à¥à¤¯à¤® à¤¸à¥‡ à¤¸à¥à¤µà¤¤à¤ƒ à¤¹à¥€ à¤¸à¥€à¤–à¤¨à¥‡ à¤”à¤° à¤­à¤µà¤¿à¤·à¥à¤¯à¤µà¤¾à¤£à¥€ à¤•à¤°à¤¨à¥‡ à¤•à¥€ à¤•à¥à¤·à¤®à¤¤à¤¾ à¤°à¤–à¤¤à¥€ à¤¹à¥ˆà¥¤\n",
    "à¤¨à¥à¤¯à¥‚à¤°à¤² à¤¨à¥‡à¤Ÿà¤µà¤°à¥à¤• à¤”à¤° à¤¡à¥€à¤ª à¤²à¤°à¥à¤¨à¤¿à¤‚à¤— à¤¤à¤•à¤¨à¥€à¤•à¥‡à¤‚ à¤®à¤¾à¤¨à¤µ à¤®à¤¸à¥à¤¤à¤¿à¤·à¥à¤• à¤•à¥€ à¤¸à¤‚à¤°à¤šà¤¨à¤¾ à¤¸à¥‡ à¤ªà¥à¤°à¥‡à¤°à¤¿à¤¤ à¤¹à¥ˆà¤‚, à¤œà¤¿à¤¨à¤®à¥‡à¤‚ à¤•à¥ƒà¤¤à¥à¤°à¤¿à¤® à¤¨à¥à¤¯à¥‚à¤°à¥‰à¤¨ à¤†à¤ªà¤¸ à¤®à¥‡à¤‚ à¤œà¤Ÿà¤¿à¤² à¤—à¤£à¤¿à¤¤à¥€à¤¯ à¤ªà¥à¤°à¤•à¥à¤°à¤¿à¤¯à¤¾à¤“à¤‚ à¤•à¥‡ à¤®à¤¾à¤§à¥à¤¯à¤® à¤¸à¥‡ à¤¸à¤‚à¤µà¤¾à¤¦ à¤•à¤°à¤¤à¥‡ à¤¹à¥à¤ à¤¨à¤¿à¤°à¥à¤£à¤¯ à¤²à¥‡à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤\"\"\"\n",
    "     \n",
    ")\n",
    "\n",
    "initial_chunk_size = 50  # Initial chunk size\n",
    "max_chunk_size = 1000    # Max chunk size\n",
    "\n",
    "generate_wav_from_text(\n",
    "    prompt=hindi_text,\n",
    "    initial_chunk_size=initial_chunk_size,\n",
    "    max_chunk_size=max_chunk_size,\n",
    "    output_file=f\"generated_audio_2_{initial_chunk_size}.wav\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cf3ce3a-26a5-41fc-8ba5-2f94e7fa7d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Audio successfully saved as 'generated_audio_3_10.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_3_20.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_3_30.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_3_40.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_3_50.wav'\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,60,10):\n",
    "    hindi_text = (\"\"\"à¤‡à¤¨ à¤¤à¤•à¤¨à¥€à¤•à¥‹à¤‚ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤‚à¤ªà¥à¤¯à¥‚à¤Ÿà¤° à¤µà¤¿à¤œà¤¼à¤¨, à¤ªà¥à¤°à¤¾à¤•à¥ƒà¤¤à¤¿à¤• à¤­à¤¾à¤·à¤¾ à¤¸à¤‚à¤¸à¤¾à¤§à¤¨ à¤”à¤° à¤¸à¥à¤µà¤šà¤¾à¤²à¤¿à¤¤ à¤µà¤¾à¤¹à¤¨ à¤ªà¥à¤°à¤£à¤¾à¤²à¥€ à¤œà¥ˆà¤¸à¥‡ à¤•à¥à¤·à¥‡à¤¤à¥à¤°à¥‹à¤‚ à¤®à¥‡à¤‚ à¤µà¥à¤¯à¤¾à¤ªà¤• à¤°à¥‚à¤ª à¤¸à¥‡ à¤•à¤¿à¤¯à¤¾ à¤œà¤¾ à¤°à¤¹à¤¾ à¤¹à¥ˆà¥¤ à¤¹à¤¾à¤² à¤¹à¥€ à¤®à¥‡à¤‚ à¤µà¤¿à¤•à¤¸à¤¿à¤¤ à¤•à¤¿à¤ à¤—à¤ à¤šà¥ˆà¤Ÿà¤¬à¥‰à¤Ÿà¥à¤¸ à¤”à¤° à¤œà¤¨à¤°à¥‡à¤Ÿà¤¿à¤µ à¤ªà¥à¤°à¥€-à¤Ÿà¥à¤°à¥‡à¤¨à¥à¤¡ à¤Ÿà¥à¤°à¤¾à¤‚à¤¸à¤«à¥‰à¤°à¥à¤®à¤° à¤®à¥‰à¤¡à¤², à¤œà¤¿à¤¨à¥à¤¹à¥‡à¤‚ GPT à¤•à¤¹à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆ, à¤®à¤¾à¤¨à¤µ à¤­à¤¾à¤·à¤¾à¤“à¤‚ à¤•à¥‹ à¤¸à¤®à¤à¤¨à¥‡ à¤¤à¤¥à¤¾ à¤‰à¤¨à¤•à¥‡ à¤¸à¤‚à¤¦à¤°à¥à¤­ à¤•à¥‡ à¤…à¤¨à¥à¤°à¥‚à¤ª à¤…à¤°à¥à¤¥à¤ªà¥‚à¤°à¥à¤£ à¤œà¤µà¤¾à¤¬ à¤‰à¤¤à¥à¤ªà¤¨à¥à¤¨ à¤•à¤°à¤¨à¥‡ à¤®à¥‡à¤‚ à¤¸à¤•à¥à¤·à¤® à¤¹à¥ˆà¤‚à¥¤ à¤¯à¤¦à¥à¤¯à¤ªà¤¿ à¤¯à¤¹ à¤¤à¤•à¤¨à¥€à¤•à¥‡à¤‚ à¤…à¤¤à¥à¤¯à¤‚à¤¤ à¤²à¤¾à¤­à¤•à¤¾à¤°à¥€ à¤¹à¥ˆà¤‚, à¤«à¤¿à¤° à¤­à¥€ à¤¡à¥‡à¤Ÿà¤¾ à¤•à¥€ à¤—à¥‹à¤ªà¤¨à¥€à¤¯à¤¤à¤¾, à¤¨à¥ˆà¤¤à¤¿à¤•à¤¤à¤¾, à¤¤à¤¥à¤¾ à¤°à¥‹à¤œà¤—à¤¾à¤° à¤ªà¤° à¤‡à¤¨à¤•à¤¾ à¤ªà¥à¤°à¤­à¤¾à¤µ à¤œà¥ˆà¤¸à¥‡ à¤œà¤Ÿà¤¿à¤² à¤ªà¥à¤°à¤¶à¥à¤¨ à¤­à¥€ à¤‰à¤¤à¥à¤ªà¤¨à¥à¤¨ à¤¹à¥‹à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤œà¤¿à¤¨à¤•à¤¾ à¤¸à¤®à¤¾à¤§à¤¾à¤¨ à¤†à¤¨à¥‡ à¤µà¤¾à¤²à¥‡ à¤¸à¤®à¤¯ à¤®à¥‡à¤‚ à¤®à¤¾à¤¨à¤µ à¤”à¤° à¤®à¤¶à¥€à¤¨ à¤•à¥‡ à¤®à¤§à¥à¤¯ à¤¸à¤‚à¤¬à¤‚à¤§à¥‹à¤‚ à¤•à¥€ à¤¦à¤¿à¤¶à¤¾ à¤¨à¤¿à¤°à¥à¤§à¤¾à¤°à¤¿à¤¤ à¤•à¤°à¥‡à¤—à¤¾à¥¤\"\"\")\n",
    "\n",
    "    initial_chunk_size = i  # Initial chunk size\n",
    "    max_chunk_size = 1000    # Max chunk size\n",
    "\n",
    "    generate_wav_from_text(\n",
    "    prompt=hindi_text,\n",
    "    initial_chunk_size=initial_chunk_size,\n",
    "    max_chunk_size=max_chunk_size,\n",
    "    output_file=f\"generated_audio_3_{initial_chunk_size}.wav\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e45e14ee-cefd-47ee-9262-5f3a0a7d5456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Audio successfully saved as 'generated_audio_4_10.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_4_20.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_4_30.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_4_40.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_4_50.wav'\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,60,10):\n",
    "    hindi_text = (\"\"\"à¤®à¤¾à¤¨à¤µ à¤¶à¤°à¥€à¤° à¤à¤• à¤…à¤¤à¥à¤¯à¤‚à¤¤ à¤œà¤Ÿà¤¿à¤² à¤¤à¤‚à¤¤à¥à¤° à¤¹à¥ˆ à¤œà¤¿à¤¸à¤®à¥‡à¤‚ à¤µà¤¿à¤­à¤¿à¤¨à¥à¤¨ à¤…à¤‚à¤— à¤†à¤ªà¤¸ à¤®à¥‡à¤‚ à¤¸à¤¾à¤®à¤‚à¤œà¤¸à¥à¤¯ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤•à¤¾à¤°à¥à¤¯ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ à¤¹à¤®à¤¾à¤°à¤¾ à¤¹à¥ƒà¤¦à¤¯ à¤°à¤•à¥à¤¤ à¤ªà¤°à¤¿à¤¸à¤‚à¤šà¤°à¤£ à¤ªà¥à¤°à¤£à¤¾à¤²à¥€ à¤•à¤¾ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤¹à¥ˆ, à¤œà¥‹ à¤ªà¥‚à¤°à¥‡ à¤¶à¤°à¥€à¤° à¤®à¥‡à¤‚ à¤‘à¤•à¥à¤¸à¥€à¤œà¤¨ à¤”à¤° à¤ªà¥‹à¤·à¤• à¤¤à¤¤à¥à¤µà¥‹à¤‚ à¤•à¥€ à¤†à¤ªà¥‚à¤°à¥à¤¤à¤¿ à¤¸à¥à¤¨à¤¿à¤¶à¥à¤šà¤¿à¤¤ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤°à¤•à¥à¤¤ à¤µà¤¾à¤¹à¤¿à¤•à¤¾à¤“à¤‚ à¤•à¤¾ à¤œà¤¾à¤² à¤§à¤®à¤¨à¤¿à¤¯à¥‹à¤‚ à¤”à¤° à¤¶à¤¿à¤°à¤¾à¤“à¤‚ à¤•à¥‡ à¤®à¤¾à¤§à¥à¤¯à¤® à¤¸à¥‡ à¤°à¤•à¥à¤¤ à¤•à¥‹ à¤¹à¤° à¤…à¤‚à¤— à¤¤à¤• à¤ªà¤¹à¥à¤à¤šà¤¾à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤‡à¤¸à¤•à¥‡ à¤…à¤²à¤¾à¤µà¤¾, à¤¤à¤‚à¤¤à¥à¤°à¤¿à¤•à¤¾ à¤¤à¤‚à¤¤à¥à¤° à¤¶à¤°à¥€à¤° à¤•à¥‡ à¤¹à¤° à¤¹à¤¿à¤¸à¥à¤¸à¥‡ à¤•à¥‹ à¤¨à¤¿à¤¯à¤‚à¤¤à¥à¤°à¤¿à¤¤ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ, à¤œà¤¿à¤¸à¤®à¥‡à¤‚ à¤®à¤¸à¥à¤¤à¤¿à¤·à¥à¤•, à¤°à¥€à¤¢à¤¼ à¤•à¥€ à¤¹à¤¡à¥à¤¡à¥€ à¤”à¤° à¤ªà¤°à¤¿à¤§à¥€à¤¯ à¤¤à¤‚à¤¤à¥à¤°à¤¿à¤•à¤¾à¤à¤ à¤ªà¥à¤°à¤®à¥à¤– à¤­à¥‚à¤®à¤¿à¤•à¤¾ à¤¨à¤¿à¤­à¤¾à¤¤à¥€ à¤¹à¥ˆà¤‚à¥¤ \"\"\")\n",
    "\n",
    "    initial_chunk_size = i  # Initial chunk size\n",
    "    max_chunk_size = 1000    # Max chunk size\n",
    "\n",
    "    generate_wav_from_text(\n",
    "    prompt=hindi_text,\n",
    "    initial_chunk_size=initial_chunk_size,\n",
    "    max_chunk_size=max_chunk_size,\n",
    "    output_file=f\"generated_audio_4_{initial_chunk_size}.wav\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "211b1a4a-7813-434e-b98d-e008ae44a99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Audio successfully saved as 'generated_audio_5_10.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_5_20.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_5_30.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_5_40.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_5_50.wav'\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,60,10):\n",
    "    hindi_text = ('''à¤à¤• à¤¸à¤®à¤¯ à¤•à¥€ à¤¬à¤¾à¤¤ à¤¹à¥ˆ, à¤¦à¥‚à¤° à¤•à¤¿à¤¸à¥€ à¤—à¤¾à¤à¤µ à¤®à¥‡à¤‚ à¤à¤• à¤¬à¥‚à¤¢à¤¼à¤¾ à¤•à¤¿à¤¸à¤¾à¤¨ à¤…à¤ªà¤¨à¥‡ à¤¬à¥‡à¤Ÿà¥‡ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤°à¤¹à¤¤à¤¾ à¤¥à¤¾à¥¤ à¤‰à¤¨à¤•à¥‡ à¤ªà¤¾à¤¸ à¤à¤• à¤˜à¥‹à¤¡à¤¼à¤¾ à¤¥à¤¾, à¤œà¥‹ à¤—à¤¾à¤à¤µ à¤­à¤° à¤®à¥‡à¤‚ à¤…à¤ªà¤¨à¥€ à¤–à¥‚à¤¬à¤¸à¥‚à¤°à¤¤à¥€ à¤”à¤° à¤¤à¤¾à¤•à¤¤ à¤•à¥‡ à¤²à¤¿à¤ à¤ªà¥à¤°à¤¸à¤¿à¤¦à¥à¤§ à¤¥à¤¾à¥¤ à¤à¤• à¤¦à¤¿à¤¨ à¤µà¤¹ à¤˜à¥‹à¤¡à¤¼à¤¾ à¤…à¤šà¤¾à¤¨à¤• à¤²à¤¾à¤ªà¤¤à¤¾ à¤¹à¥‹ à¤—à¤¯à¤¾à¥¤ à¤—à¤¾à¤à¤µ à¤•à¥‡ à¤²à¥‹à¤— à¤¯à¤¹ à¤–à¤¬à¤° à¤¸à¥à¤¨à¤•à¤° à¤•à¤¿à¤¸à¤¾à¤¨ à¤•à¥‡ à¤ªà¤¾à¤¸ à¤†à¤ à¤”à¤° à¤¬à¥‹à¤²à¥‡, \"à¤¤à¥à¤®à¥à¤¹à¤¾à¤°à¥‡ à¤¸à¤¾à¤¥ à¤¬à¤¡à¤¼à¥€ à¤¦à¥à¤°à¥à¤­à¤¾à¤—à¥à¤¯à¤ªà¥‚à¤°à¥à¤£ à¤˜à¤Ÿà¤¨à¤¾ à¤˜à¤Ÿà¥€ à¤¹à¥ˆà¥¤\" à¤•à¤¿à¤¸à¤¾à¤¨ à¤¨à¥‡ à¤¸à¤¹à¤œà¤¤à¤¾ à¤¸à¥‡ à¤œà¤µà¤¾à¤¬ à¤¦à¤¿à¤¯à¤¾, \"à¤¶à¤¾à¤¯à¤¦ à¤¯à¥‡ à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆ à¤¯à¤¾ à¤¬à¥à¤°à¤¾, à¤•à¥Œà¤¨ à¤œà¤¾à¤¨à¥‡?\" à¤•à¥à¤› à¤¦à¤¿à¤¨à¥‹à¤‚ à¤¬à¤¾à¤¦, à¤•à¤¿à¤¸à¤¾à¤¨ à¤•à¤¾ à¤˜à¥‹à¤¡à¤¼à¤¾ à¤µà¤¾à¤ªà¤¸ à¤†à¤¯à¤¾, à¤”à¤° à¤…à¤ªà¤¨à¥‡ à¤¸à¤¾à¤¥ à¤œà¤‚à¤—à¤²à¥€ à¤˜à¥‹à¤¡à¤¼à¥‹à¤‚ à¤•à¤¾ à¤à¤• à¤ªà¥‚à¤°à¤¾ à¤à¥à¤‚à¤¡ à¤²à¥‡ à¤†à¤¯à¤¾à¥¤ à¤—à¤¾à¤à¤µ à¤•à¥‡ à¤²à¥‹à¤— à¤«à¤¿à¤° à¤•à¤¿à¤¸à¤¾à¤¨ à¤•à¥‡ à¤ªà¤¾à¤¸ à¤ªà¤¹à¥à¤à¤šà¥‡ à¤”à¤° à¤¬à¥‹à¤²à¥‡, \"à¤¤à¥à¤®à¥à¤¹à¤¾à¤°à¤¾ à¤­à¤¾à¤—à¥à¤¯ à¤¬à¤¹à¥à¤¤ à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆà¥¤ à¤à¤• à¤˜à¥‹à¤¡à¤¼à¥‡ à¤•à¥‡ à¤¬à¤¦à¤²à¥‡ à¤…à¤¬ à¤¤à¥à¤®à¥à¤¹à¤¾à¤°à¥‡ à¤ªà¤¾à¤¸ à¤•à¤ˆ à¤˜à¥‹à¤¡à¤¼à¥‡ à¤† à¤—à¤ à¤¹à¥ˆà¤‚à¥¤\" à¤•à¤¿à¤¸à¤¾à¤¨ à¤«à¤¿à¤° à¤®à¥à¤¸à¥à¤•à¥à¤°à¤¾à¤¤à¥‡ à¤¹à¥à¤ à¤¬à¥‹à¤²à¤¾, \"à¤¶à¤¾à¤¯à¤¦ à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆ, à¤¶à¤¾à¤¯à¤¦ à¤¨à¤¹à¥€à¤‚, à¤•à¥Œà¤¨ à¤œà¤¾à¤¨à¥‡?''')\n",
    "\n",
    "    initial_chunk_size = i  # Initial chunk size\n",
    "    max_chunk_size = 1000    # Max chunk size\n",
    "\n",
    "    generate_wav_from_text(\n",
    "    prompt=hindi_text,\n",
    "    initial_chunk_size=initial_chunk_size,\n",
    "    max_chunk_size=max_chunk_size,\n",
    "    output_file=f\"generated_audio_5_{initial_chunk_size}.wav\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00d022f5-de8a-4396-89b0-5890e8f5dd8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`prompt_attention_mask` is specified but `attention_mask` is not. A full `attention_mask` will be created. Make sure this is the intended behaviour.\n",
      "`prompt_attention_mask` is specified but `attention_mask` is not. A full `attention_mask` will be created. Make sure this is the intended behaviour.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Audio successfully saved as 'generated_audio_6_40.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_6_50.wav'\n"
     ]
    }
   ],
   "source": [
    "for i in range(40,60,10):\n",
    "    hindi_text = ('''à¤•à¥à¤› à¤¦à¤¿à¤¨ à¤¬à¥€à¤¤à¥‡, à¤”à¤° à¤•à¤¿à¤¸à¤¾à¤¨ à¤•à¤¾ à¤¬à¥‡à¤Ÿà¤¾ à¤‰à¤¨ à¤¨à¤ à¤œà¤‚à¤—à¤²à¥€ à¤˜à¥‹à¤¡à¤¼à¥‹à¤‚ à¤•à¥‹ à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤¿à¤¤ à¤•à¤°à¤¤à¥‡ à¤¸à¤®à¤¯ à¤—à¤¿à¤° à¤ªà¤¡à¤¼à¤¾ à¤”à¤° à¤‰à¤¸à¤•à¥€ à¤Ÿà¤¾à¤à¤— à¤Ÿà¥‚à¤Ÿ à¤—à¤ˆà¥¤ à¤«à¤¿à¤° à¤¸à¥‡ à¤²à¥‹à¤— à¤¸à¤¹à¤¾à¤¨à¥à¤­à¥‚à¤¤à¤¿ à¤œà¤¤à¤¾à¤¨à¥‡ à¤†à¤ à¤”à¤° à¤¬à¥‹à¤²à¥‡, \"à¤¯à¥‡ à¤¤à¥‹ à¤¬à¤¹à¥à¤¤ à¤¬à¥à¤°à¤¾ à¤¹à¥à¤†, à¤…à¤¬ à¤¤à¥à¤®à¥à¤¹à¤¾à¤°à¤¾ à¤¬à¥‡à¤Ÿà¤¾ à¤–à¥‡à¤¤à¥€-à¤¬à¤¾à¤¡à¤¼à¥€ à¤®à¥‡à¤‚ à¤®à¤¦à¤¦ à¤¨à¤¹à¥€à¤‚ à¤•à¤° à¤ªà¤¾à¤à¤—à¤¾à¥¤\" à¤•à¤¿à¤¸à¤¾à¤¨ à¤¨à¥‡ à¤‰à¤¸à¥€ à¤¶à¤¾à¤‚à¤¤à¤¿ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤‰à¤¤à¥à¤¤à¤° à¤¦à¤¿à¤¯à¤¾, \"à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆ à¤¯à¤¾ à¤¬à¥à¤°à¤¾, à¤¯à¥‡ à¤¤à¥‹ à¤¸à¤®à¤¯ à¤¬à¤¤à¤¾à¤à¤—à¤¾à¥¤\" à¤…à¤šà¤¾à¤¨à¤• à¤—à¤¾à¤à¤µ à¤®à¥‡à¤‚ à¤¯à¥à¤¦à¥à¤§ à¤•à¥€ à¤˜à¥‹à¤·à¤£à¤¾ à¤¹à¥‹ à¤—à¤ˆà¥¤ à¤°à¤¾à¤œà¤¾ à¤•à¥‡ à¤¸à¤¿à¤ªà¤¾à¤¹à¥€ à¤—à¤¾à¤à¤µ-à¤—à¤¾à¤à¤µ à¤œà¤¾à¤•à¤° à¤¯à¥à¤µà¤•à¥‹à¤‚ à¤•à¥‹ à¤¸à¥‡à¤¨à¤¾ à¤®à¥‡à¤‚ à¤­à¤°à¥à¤¤à¥€ à¤•à¤°à¤¨à¥‡ à¤²à¤—à¥‡à¥¤ à¤•à¤¿à¤¸à¤¾à¤¨ à¤•à¥‡ à¤¬à¥‡à¤Ÿà¥‡ à¤•à¥€ à¤Ÿà¥‚à¤Ÿà¥€ à¤Ÿà¤¾à¤à¤— à¤¦à¥‡à¤–à¤•à¤° à¤‰à¤¨à¥à¤¹à¥‹à¤‚à¤¨à¥‡ à¤‰à¤¸à¥‡ à¤­à¤°à¥à¤¤à¥€ à¤¨à¤¹à¥€à¤‚ à¤•à¤¿à¤¯à¤¾à¥¤ à¤—à¤¾à¤à¤µ à¤•à¥‡ à¤¯à¥à¤µà¤• à¤¸à¥‡à¤¨à¤¾ à¤®à¥‡à¤‚ à¤—à¤, à¤œà¤¿à¤¨à¤®à¥‡à¤‚ à¤¸à¥‡ à¤•à¤ˆ à¤¯à¥à¤¦à¥à¤§ à¤¸à¥‡ à¤µà¤¾à¤ªà¤¸ à¤¨à¤¹à¥€à¤‚ à¤²à¥Œà¤Ÿà¥‡à¥¤ à¤—à¤¾à¤à¤µ à¤•à¥‡ à¤²à¥‹à¤— à¤«à¤¿à¤° à¤•à¤¿à¤¸à¤¾à¤¨ à¤•à¥‡ à¤ªà¤¾à¤¸ à¤†à¤ à¤”à¤° à¤¬à¥‹à¤²à¥‡, \"à¤¤à¥à¤®à¥à¤¹à¤¾à¤°à¥€ à¤¦à¥‚à¤°à¤¦à¤°à¥à¤¶à¤¿à¤¤à¤¾ à¤¸à¤š à¤¸à¤¾à¤¬à¤¿à¤¤ à¤¹à¥à¤ˆà¥¤ à¤¤à¥à¤®à¥à¤¹à¤¾à¤°à¤¾ à¤¬à¥‡à¤Ÿà¤¾ à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤ à¤¹à¥ˆà¥¤\" à¤•à¤¿à¤¸à¤¾à¤¨ à¤¨à¥‡ à¤«à¤¿à¤° à¤‰à¤¸à¥€ à¤¶à¤¾à¤‚à¤¤ à¤®à¥à¤¸à¥à¤•à¤¾à¤¨ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤•à¤¹à¤¾, \"à¤…à¤šà¥à¤›à¤¾ à¤¹à¥à¤† à¤¯à¤¾ à¤¬à¥à¤°à¤¾, à¤‡à¤¸à¥‡ à¤¸à¤¿à¤°à¥à¤«à¤¼ à¤¸à¤®à¤¯ à¤¹à¥€ à¤¬à¤¤à¤¾ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤\" à¤¯à¤¹ à¤•à¤¹à¤¾à¤¨à¥€ à¤¹à¤®à¥‡à¤‚ à¤¸à¤¿à¤–à¤¾à¤¤à¥€ à¤¹à¥ˆ à¤•à¤¿ à¤œà¥€à¤µà¤¨ à¤•à¥€ à¤¹à¤° à¤ªà¤°à¤¿à¤¸à¥à¤¥à¤¿à¤¤à¤¿ à¤®à¥‡à¤‚ à¤¤à¥à¤°à¤‚à¤¤ à¤«à¥ˆà¤¸à¤²à¤¾ à¤¨ à¤•à¤°à¥‡à¤‚à¥¤ à¤•à¥à¤¯à¥‹à¤‚à¤•à¤¿ à¤œà¥‹ à¤˜à¤Ÿà¤¨à¤¾ à¤†à¤œ à¤¹à¤®à¥‡à¤‚ à¤¬à¥à¤°à¥€ à¤²à¤—à¤¤à¥€ à¤¹à¥ˆ, à¤¶à¤¾à¤¯à¤¦ à¤µà¤¹à¥€ à¤•à¤² à¤¹à¤®à¤¾à¤°à¥‡ à¤²à¤¿à¤ à¤…à¤šà¥à¤›à¥€ à¤¸à¤¾à¤¬à¤¿à¤¤ à¤¹à¥‹ à¤œà¤¾à¤à¥¤''')\n",
    "\n",
    "    initial_chunk_size = i  # Initial chunk size\n",
    "    max_chunk_size = 1000    # Max chunk size\n",
    "\n",
    "    generate_wav_from_text(\n",
    "    prompt=hindi_text,\n",
    "    initial_chunk_size=initial_chunk_size,\n",
    "    max_chunk_size=max_chunk_size,\n",
    "    output_file=f\"generated_audio_6_{initial_chunk_size}.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e12242e-e8e4-453f-bbae-e724cf057b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Audio successfully saved as 'generated_audio_7_10.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_7_20.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_7_30.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_7_40.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_7_50.wav'\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,60,10):\n",
    "    hindi_text = ('''à¤®à¥à¤à¥‡ à¤ªà¤¾à¤¨à¥€ à¤ªà¥€à¤¨à¤¾ à¤¹à¥ˆà¥¤ à¤•à¥à¤¯à¤¾ à¤†à¤ª à¤®à¥‡à¤°à¥€ à¤®à¤¦à¤¦ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚? à¤†à¤œ à¤®à¥Œà¤¸à¤® à¤¬à¤¹à¥à¤¤ à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆà¥¤ à¤¯à¤¹ à¤•à¤¿à¤¤à¤¾à¤¬ à¤®à¥à¤à¥‡ à¤¬à¤¹à¥à¤¤ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆà¥¤''')\n",
    "\n",
    "    initial_chunk_size = i  # Initial chunk size\n",
    "    max_chunk_size = 1000    # Max chunk size\n",
    "\n",
    "    generate_wav_from_text(\n",
    "    prompt=hindi_text,\n",
    "    initial_chunk_size=initial_chunk_size,\n",
    "    max_chunk_size=max_chunk_size,\n",
    "    output_file=f\"generated_audio_7_{initial_chunk_size}.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b27ec5-25c9-4f01-8649-e31da5eea60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Audio successfully saved as 'generated_audio_8_10.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_8_20.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_8_30.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_8_40.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_8_50.wav'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(10,60,10):\n",
    "    hindi_text = ('''à¤šà¤²à¥‹ à¤ªà¤¾à¤°à¥à¤• à¤®à¥‡à¤‚ à¤šà¤²à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ à¤®à¥à¤à¥‡ à¤¥à¥‹à¤¡à¤¼à¥€ à¤¦à¥‡à¤° à¤¸à¥‹à¤¨à¤¾ à¤¹à¥ˆà¥¤ à¤¤à¥à¤® à¤•à¤¹à¤¾à¤ à¤œà¤¾ à¤°à¤¹à¥‡ à¤¹à¥‹?''')\n",
    "\n",
    "    initial_chunk_size = i  # Initial chunk size\n",
    "    max_chunk_size = 1000    # Max chunk size\n",
    "\n",
    "    generate_wav_from_text(\n",
    "    prompt=hindi_text,\n",
    "    initial_chunk_size=initial_chunk_size,\n",
    "    max_chunk_size=max_chunk_size,\n",
    "    output_file=f\"generated_audio_8_{initial_chunk_size}.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "697d792a-2ff7-4e7d-ab7c-056cdd2eb98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Audio successfully saved as 'generated_audio_9_10.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_9_20.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_9_30.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_9_40.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_9_50.wav'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(10,60,10):\n",
    "    hindi_text = ('''à¤µà¤¹ à¤¬à¤¹à¥à¤¤ à¤¸à¤®à¤à¤¦à¤¾à¤° à¤¬à¤šà¥à¤šà¤¾ à¤¹à¥ˆà¥¤ à¤¯à¤¹ à¤•à¤¾à¤® à¤…à¤¬ à¤ªà¥‚à¤°à¤¾ à¤¹à¥‹ à¤—à¤¯à¤¾ à¤¹à¥ˆà¥¤ à¤§à¤¨à¥à¤¯à¤µà¤¾à¤¦, à¤†à¤ªà¤¨à¥‡ à¤¸à¤®à¤¯ à¤¨à¤¿à¤•à¤¾à¤²à¤¾à¥¤''')\n",
    "\n",
    "    initial_chunk_size = i  # Initial chunk size\n",
    "    max_chunk_size = 1000    # Max chunk size\n",
    "\n",
    "    generate_wav_from_text(\n",
    "    prompt=hindi_text,\n",
    "    initial_chunk_size=initial_chunk_size,\n",
    "    max_chunk_size=max_chunk_size,\n",
    "    output_file=f\"generated_audio_9_{initial_chunk_size}.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db70c4c3-f895-466d-861b-a1479c4c5495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Audio successfully saved as 'generated_audio_10_10.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_10_20.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_10_30.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_10_40.wav'\n",
      "âœ… Audio successfully saved as 'generated_audio_10_50.wav'\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,60,10):\n",
    "    hindi_text = ('''à¤¯à¤¹ à¤¤à¥‹ à¤¬à¤¹à¥à¤¤ à¤…à¤šà¥à¤›à¥€ à¤–à¤¬à¤° à¤¹à¥ˆ! à¤®à¥à¤à¥‡ à¤†à¤œ à¤¬à¤¹à¥à¤¤ à¤–à¥à¤¶à¥€ à¤¹à¥‹ à¤°à¤¹à¥€ à¤¹à¥ˆà¥¤ à¤®à¥à¤à¥‡ à¤…à¤•à¥‡à¤²à¤¾à¤ªà¤¨ à¤®à¤¹à¤¸à¥‚à¤¸ à¤¹à¥‹ à¤°à¤¹à¤¾ à¤¹à¥ˆà¥¤ à¤†à¤œ à¤•à¤¾ à¤¦à¤¿à¤¨ à¤•à¥à¤› à¤…à¤šà¥à¤›à¤¾ à¤¨à¤¹à¥€à¤‚ à¤—à¤¯à¤¾à¥¤ à¤¤à¥à¤®à¤¨à¥‡ à¤®à¥‡à¤°à¥€ à¤¬à¤¾à¤¤ à¤•à¥à¤¯à¥‹à¤‚ à¤¨à¤¹à¥€à¤‚ à¤®à¤¾à¤¨à¥€? à¤¯à¤¹ à¤¬à¤¿à¤²à¥à¤•à¥à¤² à¤­à¥€ à¤ à¥€à¤• à¤¨à¤¹à¥€à¤‚ à¤¹à¥à¤†à¥¤''')\n",
    "\n",
    "    initial_chunk_size = i  # Initial chunk size\n",
    "    max_chunk_size = 1000    # Max chunk size\n",
    "\n",
    "    generate_wav_from_text(\n",
    "    prompt=hindi_text,\n",
    "    initial_chunk_size=initial_chunk_size,\n",
    "    max_chunk_size=max_chunk_size,\n",
    "    output_file=f\"generated_audio_10_{initial_chunk_size}.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1290c03-9b70-4174-9fd0-343a8e502401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_path  ... predict_dataset\n",
      "0  /media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254...  ...         sarulab\n",
      "1  /media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254...  ...         sarulab\n",
      "2  /media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254...  ...         sarulab\n",
      "3  /media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254...  ...         sarulab\n",
      "4  /media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254...  ...         sarulab\n",
      "\n",
      "[5 rows x 4 columns]\n",
      "Using model: ssl_multispec_ext_v2\n",
      "/home/irlab/.pyenv/versions/pyvoice/lib/python3.10/site-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Loaded weight from /home/irlab/.cache/utmosv2/models/fusion_stage3/fold0_s42_best_model.pth\n",
      "+*+*[[Fold 1/5]]+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n",
      "  [Inference] (1/1):   0%|                                | 0/7 [00:00<?, ?it/s]/media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254e15/Parth/Voice/UTMOSv2/utmosv2/runner/_inference.py:60: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "  [Inference] (1/1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:31<00:00,  4.46s/it]\n",
      "Average of 1 folds\n",
      "Predictions are saved to /media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254e15/Parth/Voice/gen_audios/result.csv\n"
     ]
    }
   ],
   "source": [
    "!python ./UTMOSv2/inference.py --input_dir /media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254e15/Parth/Voice/gen_audios --out_path /media/irlab/ba5a20df-2f59-4a88-b4ad-08a3df254e15/Parth/Voice/gen_audios/result.csv\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
